{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6e342d55-0774-44ea-afca-624d92004c39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Combined frame: (1085703, 387)\n",
      "[INFO] Saved accuracy graph -> C:\\Users\\sagni\\Downloads\\SmartMeterX\\accuracy_per_appliance.png\n",
      "[INFO] Saved heatmap -> C:\\Users\\sagni\\Downloads\\SmartMeterX\\error_heatmap_hour.png\n"
     ]
    }
   ],
   "source": [
    "import os, re, csv, json, pickle, warnings, glob\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "try:\n",
    "    import seaborn as sns\n",
    "    USE_SNS = True\n",
    "except Exception:\n",
    "    USE_SNS = False\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_absolute_error, r2_score\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "# --------------------------\n",
    "# Paths\n",
    "# --------------------------\n",
    "DATA_DIR   = r\"C:\\Users\\sagni\\Downloads\\SmartMeterX\\archive (2)\"\n",
    "OUTPUT_DIR = r\"C:\\Users\\sagni\\Downloads\\SmartMeterX\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "PREPROC_PKL = os.path.join(OUTPUT_DIR, \"preprocessor.pkl\")\n",
    "MODEL_H5    = os.path.join(OUTPUT_DIR, \"model_disagg.h5\")\n",
    "MODEL_KERAS = os.path.join(OUTPUT_DIR, \"model_disagg.keras\")  # optional\n",
    "\n",
    "# --------------------------\n",
    "# Load preprocessor (tolerant to key names)\n",
    "# --------------------------\n",
    "with open(PREPROC_PKL, \"rb\") as f:\n",
    "    preproc = pickle.load(f)\n",
    "\n",
    "def _get(dct, *keys, default=None):\n",
    "    \"\"\"Return first existing key from `keys` in dict `dct`, else `default` if provided, else raise KeyError.\"\"\"\n",
    "    for k in keys:\n",
    "        if k in dct:\n",
    "            return dct[k]\n",
    "    if default is not None:\n",
    "        return default\n",
    "    raise KeyError(f\"None of {keys} found in dict and no default provided.\")\n",
    "\n",
    "mains_col_saved = _get(preproc, \"mains_column\", \"mains_col\")\n",
    "appliance_cols  = list(_get(preproc, \"appliance_columns\", \"appliance_cols\"))\n",
    "feature_cols    = list(_get(preproc, \"feature_columns\", \"features\", \"feature_names\",\n",
    "                            default=[\"mains\",\"h_sin\",\"h_cos\",\"d_sin\",\"d_cos\"]))\n",
    "WINDOW          = int(_get(preproc, \"window\"))\n",
    "STRIDE          = int(_get(preproc, \"stride\"))\n",
    "# scalers may be saved under different keys\n",
    "scaler_X: StandardScaler = _get(preproc, \"scaler_X\", \"x_scaler\")\n",
    "scaler_Y: StandardScaler = _get(preproc, \"scaler_Y\", \"y_scaler\")\n",
    "\n",
    "# --------------------------\n",
    "# Robust model loader (avoid 'mae' alias issue in H5)\n",
    "# --------------------------\n",
    "def load_model_safe():\n",
    "    if os.path.exists(MODEL_KERAS):\n",
    "        try:\n",
    "            return tf.keras.models.load_model(MODEL_KERAS, safe_mode=False)\n",
    "        except Exception as e:\n",
    "            print(\"[WARN] Failed to load model_disagg.keras:\", e)\n",
    "    if os.path.exists(MODEL_H5):\n",
    "        try:\n",
    "            return tf.keras.models.load_model(MODEL_H5, compile=False)\n",
    "        except Exception as e:\n",
    "            try:\n",
    "                return tf.keras.models.load_model(\n",
    "                    MODEL_H5,\n",
    "                    custom_objects={\n",
    "                        \"mae\": tf.keras.metrics.mean_absolute_error,\n",
    "                        \"MeanAbsoluteError\": tf.keras.metrics.MeanAbsoluteError,\n",
    "                    },\n",
    "                    compile=False\n",
    "                )\n",
    "            except Exception as e2:\n",
    "                raise RuntimeError(f\"Could not load model from H5: {e2}\") from e\n",
    "    raise FileNotFoundError(\"No model found (model_disagg.keras or model_disagg.h5).\")\n",
    "\n",
    "model = load_model_safe()\n",
    "\n",
    "# --------------------------\n",
    "# Robust CSV loader (same style as training)\n",
    "# --------------------------\n",
    "def robust_read_csv(path, expect_min_cols=2):\n",
    "    encs = [\"utf-8\",\"utf-8-sig\",\"cp1252\",\"latin1\"]\n",
    "    seps = [\",\",\";\",\"\\t\",\"|\"]\n",
    "    try:\n",
    "        with open(path, \"rb\") as f:\n",
    "            head = f.read(8192).decode(\"latin1\", errors=\"ignore\")\n",
    "        sn = csv.Sniffer().sniff(head)\n",
    "        if sn.delimiter in seps:\n",
    "            seps = [sn.delimiter] + [s for s in seps if s != sn.delimiter]\n",
    "    except Exception:\n",
    "        pass\n",
    "    last_err = None\n",
    "    for enc in encs:\n",
    "        for sep in seps:\n",
    "            try:\n",
    "                df = pd.read_csv(path, encoding=enc, sep=sep, engine=\"python\")\n",
    "                if df.shape[1] >= expect_min_cols:\n",
    "                    return df\n",
    "            except Exception as e:\n",
    "                last_err = e\n",
    "    raise RuntimeError(f\"Could not parse {path}. Last error: {last_err}\")\n",
    "\n",
    "def pick_time_column(cols):\n",
    "    cands = [\"timestamp\",\"time\",\"datetime\",\"date\",\"ts\",\"utc\",\"localtime\",\"index\"]\n",
    "    lmap = {c.lower(): c for c in cols}\n",
    "    for c in cands:\n",
    "        if c in lmap: return lmap[c]\n",
    "    return cols[0]\n",
    "\n",
    "def parse_time_column(df, tcol):\n",
    "    s = df[tcol]\n",
    "    if pd.api.types.is_numeric_dtype(s):\n",
    "        m = s.median()\n",
    "        try:\n",
    "            if m > 10**12:   # ms epoch\n",
    "                return pd.to_datetime(s, unit=\"ms\", errors=\"coerce\")\n",
    "            elif m > 10**9:  # s epoch\n",
    "                return pd.to_datetime(s, unit=\"s\", errors=\"coerce\")\n",
    "        except Exception:\n",
    "            pass\n",
    "    return pd.to_datetime(s, errors=\"coerce\", infer_datetime_format=True)\n",
    "\n",
    "def add_prefix(df, prefix):\n",
    "    newcols = {}\n",
    "    for c in df.columns:\n",
    "        if c.lower() in [\"timestamp\",\"time\",\"datetime\",\"date\",\"ts\",\"utc\",\"localtime\",\"index\"]:\n",
    "            newcols[c] = c\n",
    "        else:\n",
    "            newcols[c] = f\"{prefix}__{c}\"\n",
    "    return df.rename(columns=newcols)\n",
    "\n",
    "# --------------------------\n",
    "# Build combined minute-indexed frame (ignore HTML)\n",
    "# --------------------------\n",
    "all_csvs = [p for p in glob.glob(os.path.join(DATA_DIR, \"**\", \"*\"), recursive=True)\n",
    "            if os.path.isfile(p) and p.lower().endswith(\".csv\")]\n",
    "if not all_csvs:\n",
    "    raise FileNotFoundError(f\"No CSVs found under {DATA_DIR}\")\n",
    "\n",
    "frames = []\n",
    "for path in all_csvs:\n",
    "    try:\n",
    "        df = robust_read_csv(path, expect_min_cols=2)\n",
    "        tcol = pick_time_column(list(df.columns))\n",
    "        df = df.dropna(subset=[tcol]).copy()\n",
    "        df[tcol] = parse_time_column(df, tcol)\n",
    "        df = df.dropna(subset=[tcol]).set_index(tcol).sort_index()\n",
    "\n",
    "        num_df = df.select_dtypes(include=[\"number\"]).copy()\n",
    "        if num_df.empty:\n",
    "            continue\n",
    "\n",
    "        stem = os.path.splitext(os.path.basename(path))[0]\n",
    "        num_df = add_prefix(num_df, stem)\n",
    "\n",
    "        # Resample to 1-minute (sum for energy-like cols, mean otherwise)\n",
    "        def agg_func(colname):\n",
    "            ln = colname.lower()\n",
    "            if any(k in ln for k in [\"wh\",\"kwh\",\"energy\",\"consumption\"]):\n",
    "                return \"sum\"\n",
    "            return \"mean\"\n",
    "        aggmap = {c: agg_func(c) for c in num_df.columns}\n",
    "        num_df = num_df.resample(\"1T\").agg(aggmap)\n",
    "\n",
    "        frames.append(num_df)\n",
    "    except Exception:\n",
    "        continue\n",
    "\n",
    "if not frames:\n",
    "    raise RuntimeError(\"No usable numeric CSVs could be parsed.\")\n",
    "\n",
    "data = pd.concat(frames, axis=1).sort_index()\n",
    "data = data.ffill().bfill().fillna(0.0)\n",
    "print(\"[INFO] Combined frame:\", data.shape)\n",
    "\n",
    "# --------------------------\n",
    "# Locate mains/appliance columns from the saved names (fuzzy match ok)\n",
    "# --------------------------\n",
    "def find_col_exact_or_fuzzy(df_cols, target_name):\n",
    "    if target_name in df_cols:\n",
    "        return target_name\n",
    "    t_low = target_name.lower()\n",
    "    for c in df_cols:\n",
    "        lc = c.lower()\n",
    "        if lc == t_low or lc.endswith(t_low) or t_low in lc:\n",
    "            return c\n",
    "    return None\n",
    "\n",
    "mains_col = find_col_exact_or_fuzzy(list(data.columns), mains_col_saved)\n",
    "if mains_col is None:\n",
    "    raise RuntimeError(f\"Saved mains column '{mains_col_saved}' not found in combined data.\")\n",
    "\n",
    "appl_cols = []\n",
    "for ap in appliance_cols:\n",
    "    c = find_col_exact_or_fuzzy(list(data.columns), ap)\n",
    "    if c is None:\n",
    "        raise RuntimeError(f\"Saved appliance column '{ap}' not found in combined data.\")\n",
    "    appl_cols.append(c)\n",
    "\n",
    "# --------------------------\n",
    "# Features (mains + calendar) & targets\n",
    "# --------------------------\n",
    "def calendar_features(index):\n",
    "    h = index.hour.values\n",
    "    d = index.dayofweek.values\n",
    "    h_sin = np.sin(2*np.pi*h/24.0)\n",
    "    h_cos = np.cos(2*np.pi*h/24.0)\n",
    "    d_sin = np.sin(2*np.pi*d/7.0)\n",
    "    d_cos = np.cos(2*np.pi*d/7.0)\n",
    "    return np.column_stack([h_sin, h_cos, d_sin, d_cos]).astype(\"float32\")\n",
    "\n",
    "features = pd.DataFrame(index=data.index)\n",
    "features[\"mains\"] = data[mains_col].astype(\"float32\")\n",
    "cal = calendar_features(features.index)\n",
    "for i, name in enumerate([\"h_sin\",\"h_cos\",\"d_sin\",\"d_cos\"]):\n",
    "    features[name] = cal[:, i]\n",
    "\n",
    "targets = data[appl_cols].astype(\"float32\").copy()\n",
    "\n",
    "# --------------------------\n",
    "# Chronological split 70/15/15\n",
    "# --------------------------\n",
    "n = len(features)\n",
    "i1 = int(n*0.70); i2 = int(n*0.85)\n",
    "X_train_df = features.iloc[:i1]\n",
    "X_val_df   = features.iloc[i1:i2]\n",
    "X_test_df  = features.iloc[i2:]\n",
    "y_train_df = targets.iloc[:i1]\n",
    "y_val_df   = targets.iloc[i1:i2]\n",
    "y_test_df  = targets.iloc[i2:]\n",
    "\n",
    "# --------------------------\n",
    "# Make windows with saved WINDOW/STRIDE, then scale using saved scalers\n",
    "# --------------------------\n",
    "def make_windows(Xdf, ydf, window=WINDOW, stride=STRIDE):\n",
    "    X = Xdf.values; Y = ydf.values\n",
    "    xs, ys, idx_starts = [], [], []\n",
    "    N = len(X)\n",
    "    for start in range(0, N - window + 1, stride):\n",
    "        end = start + window\n",
    "        xs.append(X[start:end, :])\n",
    "        ys.append(Y[start:end, :])\n",
    "        idx_starts.append(start)\n",
    "    if not xs:\n",
    "        return None, None, None\n",
    "    return np.stack(xs), np.stack(ys), np.array(idx_starts, dtype=int)\n",
    "\n",
    "Xte_win, yte_win, starts = make_windows(X_test_df, y_test_df)\n",
    "if Xte_win is None:\n",
    "    raise RuntimeError(\"Not enough test data to create windows. Reduce WINDOW/STRIDE.\")\n",
    "\n",
    "def scale_xy(Xw, Yw, scaler_X, scaler_Y):\n",
    "    X = scaler_X.transform(Xw.reshape(-1, Xw.shape[-1])).reshape(Xw.shape)\n",
    "    Y = scaler_Y.transform(Yw.reshape(-1, Yw.shape[-1])).reshape(Yw.shape)\n",
    "    return X.astype(\"float32\"), Y.astype(\"float32\")\n",
    "\n",
    "Xte_scaled, yte_scaled = scale_xy(Xte_win, yte_win, scaler_X, scaler_Y)\n",
    "\n",
    "# --------------------------\n",
    "# Predict & invert scaling\n",
    "# --------------------------\n",
    "yte_pred_scaled = model.predict(Xte_scaled, batch_size=128, verbose=0)\n",
    "Dout = yte_scaled.shape[-1]\n",
    "\n",
    "yte_pred_flat = yte_pred_scaled.reshape(-1, Dout)\n",
    "yte_true_flat = yte_scaled.reshape(-1, Dout)\n",
    "yte_pred_unscaled = scaler_Y.inverse_transform(yte_pred_flat)\n",
    "yte_true_unscaled = scaler_Y.inverse_transform(yte_true_flat)\n",
    "yte_pred_unscaled = np.clip(yte_pred_unscaled, 0.0, None)\n",
    "\n",
    "# --------------------------\n",
    "# 1) Accuracy graph: R² per appliance\n",
    "# --------------------------\n",
    "r2_per_app = {}\n",
    "for i, ap in enumerate(appliance_cols):\n",
    "    y_true = yte_true_unscaled[:, i]\n",
    "    y_pred = yte_pred_unscaled[:, i]\n",
    "    try:\n",
    "        r2 = r2_score(y_true, y_pred)\n",
    "    except Exception:\n",
    "        r2 = np.nan\n",
    "    if not np.isfinite(r2):\n",
    "        r2 = 0.0\n",
    "    r2_per_app[appliance_cols[i]] = float(r2)\n",
    "\n",
    "plt.figure(figsize=(10, 4))\n",
    "apps = list(r2_per_app.keys())\n",
    "vals = [r2_per_app[a] for a in apps]\n",
    "if USE_SNS:\n",
    "    ax = sns.barplot(x=apps, y=vals)\n",
    "    for p, v in zip(ax.patches, vals):\n",
    "        ax.annotate(f\"{v:.2f}\", (p.get_x()+p.get_width()/2, p.get_height()),\n",
    "                    ha='center', va='bottom', xytext=(0,3), textcoords='offset points', fontsize=9)\n",
    "else:\n",
    "    ax = plt.bar(apps, vals)\n",
    "    for j, v in enumerate(vals):\n",
    "        plt.text(j, v + 0.01, f\"{v:.2f}\", ha=\"center\", va=\"bottom\", fontsize=9)\n",
    "plt.xticks(rotation=30, ha=\"right\")\n",
    "plt.ylabel(\"R² (higher is better)\")\n",
    "plt.title(\"Disaggregation Accuracy per Appliance (R² on Test)\")\n",
    "plt.ylim(0, max(0.05, max(vals)+0.1))\n",
    "plt.tight_layout()\n",
    "acc_path = os.path.join(OUTPUT_DIR, \"accuracy_per_appliance.png\")\n",
    "plt.savefig(acc_path, dpi=150); plt.close()\n",
    "print(f\"[INFO] Saved accuracy graph -> {acc_path}\")\n",
    "\n",
    "# --------------------------\n",
    "# 2) Heatmap: MAE by hour-of-day (reconstruct minute-level predictions)\n",
    "# --------------------------\n",
    "# Reconstruct minute-level predictions by overlap-averaging window outputs\n",
    "T_test   = len(X_test_df)              # minutes in test segment\n",
    "M        = len(appliance_cols)\n",
    "pred_sum = np.zeros((T_test, M), dtype=np.float64)\n",
    "pred_cnt = np.zeros((T_test, M), dtype=np.float64)\n",
    "\n",
    "for k, start in enumerate(starts):\n",
    "    end = start + WINDOW\n",
    "    sl = slice(k*WINDOW, (k+1)*WINDOW)\n",
    "    pred_sum[start:end, :] += yte_pred_unscaled[sl, :]\n",
    "    pred_cnt[start:end, :] += 1.0\n",
    "\n",
    "pred_cnt[pred_cnt == 0] = 1.0\n",
    "pred_series = pred_sum / pred_cnt\n",
    "\n",
    "true_series = y_test_df.values.astype(np.float64)[:T_test, :]\n",
    "idx_test    = y_test_df.index[:T_test]\n",
    "\n",
    "# Compute MAE per hour for each appliance\n",
    "hours = idx_test.hour.values\n",
    "heat = np.zeros((M, 24), dtype=np.float64)\n",
    "for i in range(M):\n",
    "    for h in range(24):\n",
    "        sel = (hours == h)\n",
    "        if sel.any():\n",
    "            err = np.abs(true_series[sel, i] - pred_series[sel, i]).mean()\n",
    "        else:\n",
    "            err = np.nan\n",
    "        heat[i, h] = err\n",
    "\n",
    "plt.figure(figsize=(12, 0.6*M + 2))\n",
    "if USE_SNS:\n",
    "    sns.heatmap(heat, annot=False, cmap=\"viridis\", cbar=True,\n",
    "                xticklabels=[str(h) for h in range(24)],\n",
    "                yticklabels=appliance_cols)\n",
    "else:\n",
    "    plt.imshow(np.nan_to_num(heat, nan=np.nanmean(heat)), aspect=\"auto\", cmap=\"viridis\")\n",
    "    plt.xticks(range(24), [str(h) for h in range(24)])\n",
    "    plt.yticks(range(M), appliance_cols)\n",
    "plt.xlabel(\"Hour of Day\")\n",
    "plt.ylabel(\"Appliance\")\n",
    "plt.title(\"MAE Heatmap by Hour (Test)\")\n",
    "plt.tight_layout()\n",
    "heat_path = os.path.join(OUTPUT_DIR, \"error_heatmap_hour.png\")\n",
    "plt.savefig(heat_path, dpi=150); plt.close()\n",
    "print(f\"[INFO] Saved heatmap -> {heat_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b18c0d9d-af90-4ca8-83c3-1b2b4b6d5ec2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11 (moviepy)",
   "language": "python",
   "name": "py311"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
