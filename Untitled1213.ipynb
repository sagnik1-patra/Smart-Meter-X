{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e294add1-1e56-43e5-8713-cbc7c04398b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Selected mains series from 4 matching files. Length=1051200 minutes.\n",
      "[INFO] Forecast windows: train=4000, val=1000, test=1000\n",
      "Epoch 1/5\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m412s\u001b[0m 1s/step - loss: 0.5720 - val_loss: 0.5595\n",
      "Epoch 2/5\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m320s\u001b[0m 1s/step - loss: 0.5645 - val_loss: 0.5590\n",
      "Epoch 3/5\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m308s\u001b[0m 1s/step - loss: 0.5641 - val_loss: 0.5589\n",
      "Epoch 4/5\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m305s\u001b[0m 1s/step - loss: 0.5639 - val_loss: 0.5588\n",
      "Epoch 5/5\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m298s\u001b[0m 1s/step - loss: 0.5637 - val_loss: 0.5586\n",
      "[INFO] Forecast metrics: MAE=612.383, sMAPE=40.91%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Saved forecast_plot.png -> C:\\Users\\sagni\\Downloads\\SmartMeterX\\forecast_plot.png\n",
      "[INFO] AE windows: train=10000, val=3000, test=3000\n",
      "Epoch 1/5\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 13ms/step - loss: 0.1874 - val_loss: 0.0597\n",
      "Epoch 2/5\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step - loss: 0.0383 - val_loss: 0.0236\n",
      "Epoch 3/5\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - loss: 0.0171 - val_loss: 0.0160\n",
      "Epoch 4/5\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - loss: 0.0118 - val_loss: 0.0134\n",
      "Epoch 5/5\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - loss: 0.0098 - val_loss: 0.0114\n",
      "[INFO] Anomaly threshold @ 99.0th pct: 0.096009\n",
      "[INFO] Saved anomalies.csv -> C:\\Users\\sagni\\Downloads\\SmartMeterX\\anomalies.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Saved anomaly_plot.png -> C:\\Users\\sagni\\Downloads\\SmartMeterX\\anomaly_plot.png\n",
      "\n",
      "[DONE]\n",
      " Forecast artifacts:\n",
      "  - model_forecast.h5\n",
      "  - preprocessor_forecast.pkl\n",
      "  - forecast_config.yaml\n",
      "  - metrics_forecast.json\n",
      "  - forecast_plot.png\n",
      " Anomaly artifacts:\n",
      "  - model_anomaly.h5\n",
      "  - preprocessor_anomaly.pkl\n",
      "  - threshold_anom.json\n",
      "  - anomalies.csv\n",
      "  - anomaly_plot.png\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# SmartMeterX — Memory-safe Forecast (5 epochs) + Anomaly (5 epochs)\n",
    "# Streams only the mains series; caps window counts to avoid OOM.\n",
    "# Saves artifacts to C:\\Users\\sagni\\Downloads\\SmartMeterX\n",
    "# ============================================================\n",
    "import os, re, csv, json, pickle, warnings, glob, random\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "try:\n",
    "    import seaborn as sns\n",
    "    USE_SNS = True\n",
    "except Exception:\n",
    "    USE_SNS = False\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# --------------------------\n",
    "# Paths\n",
    "# --------------------------\n",
    "DATA_DIR   = r\"C:\\Users\\sagni\\Downloads\\SmartMeterX\\archive (2)\"\n",
    "OUTPUT_DIR = r\"C:\\Users\\sagni\\Downloads\\SmartMeterX\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "PREPROC_DISAGG = os.path.join(OUTPUT_DIR, \"preprocessor.pkl\")  # from disaggregation step\n",
    "\n",
    "# --------------------------\n",
    "# Config (5 epochs, memory caps)\n",
    "# --------------------------\n",
    "SEED = 42\n",
    "np.random.seed(SEED); random.seed(SEED); tf.random.set_seed(SEED)\n",
    "\n",
    "# Forecast config\n",
    "HIST_MINUTES     = 720        # 12h history\n",
    "HORIZON_MINUTES  = 1440       # next 24h\n",
    "STRIDE_FCST      = 60         # hop between forecast windows (min) -> larger hop = fewer windows\n",
    "MAX_TR_WIN_FCST  = 4000       # cap windows to avoid OOM\n",
    "MAX_VA_WIN_FCST  = 1000\n",
    "MAX_TE_WIN_FCST  = 1000\n",
    "BATCH_FCST       = 16\n",
    "EPOCHS_FCST      = 5\n",
    "PATIENCE_FCST    = 2\n",
    "\n",
    "# Anomaly config (autoencoder on mains)\n",
    "AE_WINDOW        = 256\n",
    "AE_STRIDE        = 32\n",
    "MAX_TR_WIN_AE    = 10000\n",
    "MAX_VA_WIN_AE    = 3000\n",
    "MAX_TE_WIN_AE    = 3000\n",
    "BATCH_AE         = 64\n",
    "EPOCHS_AE        = 5\n",
    "PATIENCE_AE      = 2\n",
    "THRESHOLD_PCTL   = 99.0\n",
    "\n",
    "# --------------------------\n",
    "# Robust CSV utilities\n",
    "# --------------------------\n",
    "def robust_read_csv(path, expect_min_cols=2):\n",
    "    encs = [\"utf-8\",\"utf-8-sig\",\"cp1252\",\"latin1\"]\n",
    "    seps = [\",\",\";\",\"\\t\",\"|\"]\n",
    "    try:\n",
    "        with open(path, \"rb\") as f:\n",
    "            head = f.read(8192).decode(\"latin1\", errors=\"ignore\")\n",
    "        sn = csv.Sniffer().sniff(head)\n",
    "        if sn.delimiter in seps:\n",
    "            seps = [sn.delimiter] + [s for s in seps if s != sn.delimiter]\n",
    "    except Exception:\n",
    "        pass\n",
    "    last_err = None\n",
    "    for enc in encs:\n",
    "        for sep in seps:\n",
    "            try:\n",
    "                df = pd.read_csv(path, encoding=enc, sep=sep, engine=\"python\")\n",
    "                if df.shape[1] >= expect_min_cols:\n",
    "                    return df\n",
    "            except Exception as e:\n",
    "                last_err = e\n",
    "    raise RuntimeError(f\"Could not parse {path}. Last error: {last_err}\")\n",
    "\n",
    "def pick_time_column(cols):\n",
    "    cands = [\"timestamp\",\"time\",\"datetime\",\"date\",\"ts\",\"utc\",\"localtime\",\"index\"]\n",
    "    lmap = {c.lower(): c for c in cols}\n",
    "    for c in cands:\n",
    "        if c in lmap: return lmap[c]\n",
    "    return cols[0]\n",
    "\n",
    "def parse_time_column(df, tcol):\n",
    "    s = df[tcol]\n",
    "    if pd.api.types.is_numeric_dtype(s):\n",
    "        m = s.median()\n",
    "        try:\n",
    "            if m > 10**12:   # ms epoch\n",
    "                return pd.to_datetime(s, unit=\"ms\", errors=\"coerce\")\n",
    "            elif m > 10**9:  # s epoch\n",
    "                return pd.to_datetime(s, unit=\"s\", errors=\"coerce\")\n",
    "        except Exception:\n",
    "            pass\n",
    "    return pd.to_datetime(s, errors=\"coerce\", infer_datetime_format=True)\n",
    "\n",
    "# --------------------------\n",
    "# Load mains name saved by disaggregation\n",
    "# --------------------------\n",
    "with open(PREPROC_DISAGG, \"rb\") as f:\n",
    "    preproc_disagg = pickle.load(f)\n",
    "mains_saved = preproc_disagg.get(\"mains_column\", preproc_disagg.get(\"mains_col\"))\n",
    "if mains_saved is None:\n",
    "    raise RuntimeError(\"mains_column not found in preprocessor.pkl from disaggregation step.\")\n",
    "mains_saved_low = mains_saved.lower()\n",
    "\n",
    "# --------------------------\n",
    "# Memory-safe mains loader:\n",
    "#   scan CSVs, pick ONLY columns that match mains_saved (fuzzy),\n",
    "#   resample to 1-min, keep the strongest candidate (max total energy).\n",
    "# --------------------------\n",
    "def looks_like_energy(colname:str):\n",
    "    ln = colname.lower()\n",
    "    return any(k in ln for k in [\"wh\",\"kwh\",\"energy\",\"consumption\",\"power\",\"mains\",\"aggregate\",\"total\",\"house\",\"use\"])\n",
    "\n",
    "def load_mains_series_only(data_dir, mains_saved_low):\n",
    "    all_csvs = [p for p in glob.glob(os.path.join(data_dir, \"**\", \"*\"), recursive=True)\n",
    "                if os.path.isfile(p) and p.lower().endswith(\".csv\")]\n",
    "    if not all_csvs:\n",
    "        raise FileNotFoundError(f\"No CSVs under {data_dir}\")\n",
    "\n",
    "    best_series = None\n",
    "    best_sum = -1.0\n",
    "    matched_files = 0\n",
    "\n",
    "    for path in all_csvs:\n",
    "        try:\n",
    "            df = robust_read_csv(path, expect_min_cols=2)\n",
    "            tcol = pick_time_column(list(df.columns))\n",
    "            if tcol not in df.columns: continue\n",
    "            df = df.dropna(subset=[tcol]).copy()\n",
    "            df[tcol] = parse_time_column(df, tcol)\n",
    "            df = df.dropna(subset=[tcol]).set_index(tcol).sort_index()\n",
    "            num_df = df.select_dtypes(include=[\"number\"])\n",
    "            if num_df.empty: \n",
    "                continue\n",
    "\n",
    "            # fuzzy match mains column(s) within THIS file\n",
    "            cand_cols = [c for c in num_df.columns\n",
    "                         if (mains_saved_low in c.lower()) or c.lower().endswith(mains_saved_low)]\n",
    "            # If none matched, optionally try heuristic \"mains-like\" column names (keep only top-1 of file)\n",
    "            if not cand_cols:\n",
    "                cand_cols = [c for c in num_df.columns if looks_like_energy(c)]\n",
    "                if len(cand_cols) > 5:\n",
    "                    # keep the 5 with highest variance to avoid tiny sensors\n",
    "                    var = num_df[cand_cols].var().sort_values(ascending=False)\n",
    "                    cand_cols = list(var.index[:5])\n",
    "\n",
    "            for c in cand_cols:\n",
    "                s = num_df[c].astype(\"float32\").copy()\n",
    "                # resample to 1-min (sum for energy-ish names; mean otherwise)\n",
    "                if looks_like_energy(c):\n",
    "                    s = s.resample(\"1T\").sum().astype(\"float32\")\n",
    "                else:\n",
    "                    s = s.resample(\"1T\").mean().astype(\"float32\")\n",
    "                tot = float(np.nansum(s.values))\n",
    "                if np.isfinite(tot) and tot > best_sum:\n",
    "                    best_sum = tot\n",
    "                    best_series = s\n",
    "                    matched_files += 1\n",
    "        except Exception:\n",
    "            continue\n",
    "\n",
    "    if best_series is None:\n",
    "        raise RuntimeError(\"Could not find a mains-like series in your CSVs. Check column names.\")\n",
    "\n",
    "    best_series = best_series.sort_index().ffill().bfill().fillna(0.0).astype(\"float32\")\n",
    "    print(f\"[INFO] Selected mains series from {matched_files} matching files. Length={len(best_series)} minutes.\")\n",
    "    return best_series\n",
    "\n",
    "mains = load_mains_series_only(DATA_DIR, mains_saved_low)\n",
    "\n",
    "# --------------------------\n",
    "# Build features (mains + calendar)\n",
    "# --------------------------\n",
    "idx = mains.index\n",
    "h = idx.hour.values\n",
    "d = idx.dayofweek.values\n",
    "feat_fcst = pd.DataFrame({\n",
    "    \"mains\": mains.values,\n",
    "    \"h_sin\":  np.sin(2*np.pi*h/24.0).astype(\"float32\"),\n",
    "    \"h_cos\":  np.cos(2*np.pi*h/24.0).astype(\"float32\"),\n",
    "    \"d_sin\":  np.sin(2*np.pi*d/7.0).astype(\"float32\"),\n",
    "    \"d_cos\":  np.cos(2*np.pi*d/7.0).astype(\"float32\"),\n",
    "}, index=idx)\n",
    "\n",
    "# Chronological split 70/15/15\n",
    "n = len(feat_fcst)\n",
    "i1 = int(n*0.70); i2 = int(n*0.85)\n",
    "tr_df = feat_fcst.iloc[:i1]\n",
    "va_df = feat_fcst.iloc[i1:i2]\n",
    "te_df = feat_fcst.iloc[i2:]\n",
    "\n",
    "# --------------------------\n",
    "# Utility: create window start indices with cap\n",
    "# --------------------------\n",
    "def window_starts(N, hist, horizon, stride):\n",
    "    last = N - hist - horizon\n",
    "    if last < 0: return np.array([], dtype=int)\n",
    "    return np.arange(0, last+1, stride, dtype=int)\n",
    "\n",
    "def subsample_indices(starts, cap, seed=SEED):\n",
    "    if len(starts) <= cap: return starts\n",
    "    rng = np.random.default_rng(seed)\n",
    "    return np.sort(rng.choice(starts, size=cap, replace=False))\n",
    "\n",
    "# --------------------------\n",
    "# Forecast windows (history -> horizon), memory-capped\n",
    "# --------------------------\n",
    "def make_forecast_arrays(df, hist_len, horizon, stride, cap):\n",
    "    arr = df.values.astype(\"float32\")\n",
    "    starts = window_starts(len(arr), hist_len, horizon, stride)\n",
    "    if len(starts) == 0: return None, None\n",
    "    starts = subsample_indices(starts, cap)\n",
    "    X = np.empty((len(starts), hist_len, arr.shape[1]), dtype=\"float32\")\n",
    "    Y = np.empty((len(starts), horizon, 1), dtype=\"float32\")\n",
    "    for i, st in enumerate(starts):\n",
    "        X[i] = arr[st:st+hist_len, :]\n",
    "        Y[i, :, 0] = arr[st+hist_len:st+hist_len+horizon, 0]  # mains is col 0\n",
    "    return X, Y\n",
    "\n",
    "Xtr_f, Ytr_f = make_forecast_arrays(tr_df, HIST_MINUTES, HORIZON_MINUTES, STRIDE_FCST, MAX_TR_WIN_FCST)\n",
    "Xva_f, Yva_f = make_forecast_arrays(va_df, HIST_MINUTES, HORIZON_MINUTES, STRIDE_FCST, MAX_VA_WIN_FCST)\n",
    "Xte_f, Yte_f = make_forecast_arrays(te_df, HIST_MINUTES, HORIZON_MINUTES, STRIDE_FCST, MAX_TE_WIN_FCST)\n",
    "if Xtr_f is None or Xva_f is None or Xte_f is None:\n",
    "    raise RuntimeError(\"Not enough data to build forecast windows. Reduce HIST/HORIZON or stride.\")\n",
    "\n",
    "print(f\"[INFO] Forecast windows: train={len(Xtr_f)}, val={len(Xva_f)}, test={len(Xte_f)}\")\n",
    "\n",
    "# Scale (fit on train only)\n",
    "scaler_fcst_X = StandardScaler().fit(Xtr_f.reshape(-1, Xtr_f.shape[-1]))\n",
    "scaler_fcst_Y = StandardScaler().fit(Ytr_f.reshape(-1, 1))\n",
    "\n",
    "def scale_fcst(X, Y):\n",
    "    Xs = scaler_fcst_X.transform(X.reshape(-1, X.shape[-1])).reshape(X.shape).astype(\"float32\")\n",
    "    Ys = scaler_fcst_Y.transform(Y.reshape(-1, 1)).reshape(Y.shape).astype(\"float32\")\n",
    "    return Xs, Ys\n",
    "\n",
    "Xtr_s, Ytr_s = scale_fcst(Xtr_f, Ytr_f)\n",
    "Xva_s, Yva_s = scale_fcst(Xva_f, Yva_f)\n",
    "Xte_s, Yte_s = scale_fcst(Xte_f, Yte_f)\n",
    "\n",
    "# --------------------------\n",
    "# Forecaster model (5 epochs)\n",
    "# --------------------------\n",
    "def build_forecaster(hist_len, d_in, horizon):\n",
    "    inp = keras.Input(shape=(hist_len, d_in))\n",
    "    x = layers.LSTM(64, return_sequences=False)(inp)\n",
    "    x = layers.Dense(128, activation=\"relu\")(x)\n",
    "    x = layers.RepeatVector(horizon)(x)\n",
    "    x = layers.LSTM(64, return_sequences=True)(x)\n",
    "    out = layers.TimeDistributed(layers.Dense(1, activation=None))(x)\n",
    "    model = keras.Model(inp, out, name=\"smartmeterx_forecaster\")\n",
    "    model.compile(optimizer=keras.optimizers.Adam(1e-3), loss=\"mae\")\n",
    "    return model\n",
    "\n",
    "model_fcst = build_forecaster(HIST_MINUTES, feat_fcst.shape[1], HORIZON_MINUTES)\n",
    "early_fcst = keras.callbacks.EarlyStopping(monitor=\"val_loss\", mode=\"min\", patience=PATIENCE_FCST, restore_best_weights=True)\n",
    "hist_fcst = model_fcst.fit(\n",
    "    Xtr_s, Ytr_s,\n",
    "    validation_data=(Xva_s, Yva_s),\n",
    "    epochs=EPOCHS_FCST, batch_size=BATCH_FCST,\n",
    "    callbacks=[early_fcst], verbose=1\n",
    ")\n",
    "\n",
    "# Predict & invert scaling\n",
    "Yte_pred_s = model_fcst.predict(Xte_s, batch_size=BATCH_FCST, verbose=0)\n",
    "Yte_pred = scaler_fcst_Y.inverse_transform(Yte_pred_s.reshape(-1, 1)).reshape(Yte_pred_s.shape).astype(\"float32\")\n",
    "Yte_true = scaler_fcst_Y.inverse_transform(Yte_s.reshape(-1, 1)).reshape(Yte_s.shape).astype(\"float32\")\n",
    "\n",
    "def smape(a, f, eps=1e-6):\n",
    "    return 100.0 * np.mean(2.0 * np.abs(f - a) / (np.abs(a) + np.abs(f) + eps))\n",
    "\n",
    "mae_fcst = float(mean_absolute_error(Yte_true.ravel(), Yte_pred.ravel()))\n",
    "smape_fcst = float(smape(Yte_true.ravel(), Yte_pred.ravel()))\n",
    "with open(os.path.join(OUTPUT_DIR, \"metrics_forecast.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump({\"mae\": mae_fcst, \"smape\": smape_fcst,\n",
    "               \"hist_minutes\": HIST_MINUTES, \"horizon_minutes\": HORIZON_MINUTES,\n",
    "               \"train_windows\": int(len(Xtr_f)), \"val_windows\": int(len(Xva_f)), \"test_windows\": int(len(Xte_f))}, f, indent=2)\n",
    "print(f\"[INFO] Forecast metrics: MAE={mae_fcst:.3f}, sMAPE={smape_fcst:.2f}%\")\n",
    "\n",
    "# Example forecast plot (use first test window)\n",
    "idx_ex = 0\n",
    "t0_start = te_df.index[0]\n",
    "t_hist = pd.date_range(t0_start, periods=HIST_MINUTES, freq=\"T\")\n",
    "t_horz = pd.date_range(t_hist[-1] + pd.Timedelta(minutes=1), periods=HORIZON_MINUTES, freq=\"T\")\n",
    "x_hist = scaler_fcst_Y.inverse_transform(Yte_s[idx_ex,:,0].reshape(-1,1)).ravel()  # NOTE: Y scaler uses mains; use X mains if preferred\n",
    "# For clarity, use actual mains history from original df:\n",
    "x_hist = te_df[\"mains\"].iloc[:HIST_MINUTES].values.astype(\"float32\")\n",
    "\n",
    "y_true_plot = Yte_true[idx_ex, :, 0]\n",
    "y_pred_plot = Yte_pred[idx_ex, :, 0]\n",
    "\n",
    "plt.figure(figsize=(12,4))\n",
    "plt.plot(t_hist, x_hist, label=\"History (mains)\")\n",
    "plt.plot(t_horz, y_true_plot, label=\"True next 24h\")\n",
    "plt.plot(t_horz, y_pred_plot, label=\"Pred next 24h\")\n",
    "plt.legend(); plt.title(\"Next-24h Forecast (example)\")\n",
    "plt.xlabel(\"Time\"); plt.ylabel(\"Mains\")\n",
    "plt.tight_layout()\n",
    "fplot = os.path.join(OUTPUT_DIR, \"forecast_plot.png\")\n",
    "plt.savefig(fplot, dpi=150); plt.close()\n",
    "print(f\"[INFO] Saved forecast_plot.png -> {fplot}\")\n",
    "\n",
    "# Save forecaster + preprocessor + YAML\n",
    "model_fcst.save(os.path.join(OUTPUT_DIR, \"model_forecast.h5\"))\n",
    "preproc_fcst = {\n",
    "    \"hist_minutes\": HIST_MINUTES,\n",
    "    \"horizon_minutes\": HORIZON_MINUTES,\n",
    "    \"stride_minutes\": STRIDE_FCST,\n",
    "    \"feature_names\": [\"mains\",\"h_sin\",\"h_cos\",\"d_sin\",\"d_cos\"],\n",
    "    \"mains_column_hint\": mains_saved,          # original hint from disagg preproc\n",
    "    \"scaler_X\": scaler_fcst_X,\n",
    "    \"scaler_Y\": scaler_fcst_Y\n",
    "}\n",
    "with open(os.path.join(OUTPUT_DIR, \"preprocessor_forecast.pkl\"), \"wb\") as f:\n",
    "    pickle.dump(preproc_fcst, f)\n",
    "\n",
    "fcst_cfg = {\n",
    "    \"model\": \"smartmeterx_forecaster (LSTM seq2seq)\",\n",
    "    \"history_minutes\": HIST_MINUTES,\n",
    "    \"horizon_minutes\": HORIZON_MINUTES,\n",
    "    \"stride_minutes\": STRIDE_FCST,\n",
    "    \"optimizer\": \"Adam(1e-3)\", \"loss\": \"MAE\",\n",
    "    \"epochs\": EPOCHS_FCST, \"batch_size\": BATCH_FCST,\n",
    "    \"early_stopping\": {\"monitor\":\"val_loss\",\"mode\":\"min\",\"patience\": PATIENCE_FCST},\n",
    "    \"features\": [\"mains\",\"h_sin\",\"h_cos\",\"d_sin\",\"d_cos\"]\n",
    "}\n",
    "try:\n",
    "    import yaml\n",
    "    with open(os.path.join(OUTPUT_DIR, \"forecast_config.yaml\"), \"w\", encoding=\"utf-8\") as f:\n",
    "        yaml.safe_dump(fcst_cfg, f, sort_keys=False)\n",
    "except Exception:\n",
    "    with open(os.path.join(OUTPUT_DIR, \"forecast_config.yaml\"), \"w\", encoding=\"utf-8\") as f:\n",
    "        for k,v in fcst_cfg.items(): f.write(f\"{k}: {v}\\n\")\n",
    "\n",
    "# ============================================================\n",
    "# Unsupervised anomaly detection (autoencoder on mains) — capped windows\n",
    "# ============================================================\n",
    "series = mains.astype(\"float32\").values\n",
    "n = len(series); i1 = int(n*0.70); i2 = int(n*0.85)\n",
    "s_tr = series[:i1]; s_va = series[i1:i2]; s_te = series[i2:]\n",
    "\n",
    "def ae_window_starts(N, win, stride):\n",
    "    last = N - win\n",
    "    if last < 0: return np.array([], dtype=int)\n",
    "    return np.arange(0, last+1, stride, dtype=int)\n",
    "\n",
    "def make_ae_arrays(vec, win, stride, cap):\n",
    "    starts = ae_window_starts(len(vec), win, stride)\n",
    "    if len(starts) == 0: return None\n",
    "    starts = subsample_indices(starts, cap)\n",
    "    X = np.empty((len(starts), win, 1), dtype=\"float32\")\n",
    "    for i, st in enumerate(starts):\n",
    "        X[i,:,0] = vec[st:st+win]\n",
    "    return X, starts\n",
    "\n",
    "Xtr_ae, st_tr = make_ae_arrays(s_tr, AE_WINDOW, AE_STRIDE, MAX_TR_WIN_AE)\n",
    "Xva_ae, st_va = make_ae_arrays(s_va, AE_WINDOW, AE_STRIDE, MAX_VA_WIN_AE)\n",
    "Xte_ae, st_te = make_ae_arrays(s_te, AE_WINDOW, AE_STRIDE, MAX_TE_WIN_AE)\n",
    "if Xtr_ae is None or Xva_ae is None or Xte_ae is None:\n",
    "    raise RuntimeError(\"Not enough data to build AE windows. Reduce AE_WINDOW or stride.\")\n",
    "\n",
    "print(f\"[INFO] AE windows: train={len(Xtr_ae)}, val={len(Xva_ae)}, test={len(Xte_ae)}\")\n",
    "\n",
    "# Scale on train\n",
    "scaler_ae = StandardScaler().fit(Xtr_ae.reshape(-1,1))\n",
    "def scale_ae(X):\n",
    "    return scaler_ae.transform(X.reshape(-1,1)).reshape(X.shape).astype(\"float32\")\n",
    "\n",
    "Xtr_ae_s = scale_ae(Xtr_ae)\n",
    "Xva_ae_s = scale_ae(Xva_ae)\n",
    "Xte_ae_s = scale_ae(Xte_ae)\n",
    "\n",
    "def build_autoencoder(win=AE_WINDOW):\n",
    "    inp = keras.Input(shape=(win,1))\n",
    "    x = layers.Conv1D(32, 5, padding=\"same\", activation=\"relu\")(inp)\n",
    "    x = layers.MaxPool1D(2)(x)\n",
    "    x = layers.Conv1D(48, 5, padding=\"same\", activation=\"relu\")(x)\n",
    "    x = layers.MaxPool1D(2)(x)\n",
    "    x = layers.Conv1D(64, 3, padding=\"same\", activation=\"relu\")(x)\n",
    "    x = layers.UpSampling1D(2)(x)\n",
    "    x = layers.Conv1D(48, 5, padding=\"same\", activation=\"relu\")(x)\n",
    "    x = layers.UpSampling1D(2)(x)\n",
    "    out = layers.Conv1D(1, 3, padding=\"same\", activation=None)(x)\n",
    "    model = keras.Model(inp, out, name=\"smartmeterx_autoencoder\")\n",
    "    model.compile(optimizer=keras.optimizers.Adam(1e-3), loss=\"mse\")\n",
    "    return model\n",
    "\n",
    "model_ae = build_autoencoder()\n",
    "early_ae = keras.callbacks.EarlyStopping(monitor=\"val_loss\", mode=\"min\", patience=PATIENCE_AE, restore_best_weights=True)\n",
    "hist_ae = model_ae.fit(\n",
    "    Xtr_ae_s, Xtr_ae_s,\n",
    "    validation_data=(Xva_ae_s, Xva_ae_s),\n",
    "    epochs=EPOCHS_AE, batch_size=BATCH_AE,\n",
    "    callbacks=[early_ae], verbose=1\n",
    ")\n",
    "\n",
    "# Threshold from val\n",
    "va_rec = model_ae.predict(Xva_ae_s, verbose=0)\n",
    "va_err = np.mean((va_rec - Xva_ae_s)**2, axis=(1,2))\n",
    "thresh = float(np.percentile(va_err, THRESHOLD_PCTL))\n",
    "with open(os.path.join(OUTPUT_DIR, \"threshold_anom.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump({\"percentile\": THRESHOLD_PCTL, \"threshold\": thresh}, f, indent=2)\n",
    "print(f\"[INFO] Anomaly threshold @ {THRESHOLD_PCTL}th pct: {thresh:.6f}\")\n",
    "\n",
    "# Test scoring\n",
    "te_rec = model_ae.predict(Xte_ae_s, verbose=0)\n",
    "te_err = np.mean((te_rec - Xte_ae_s)**2, axis=(1,2))\n",
    "is_anom = (te_err >= thresh).astype(int)\n",
    "\n",
    "# Map to timestamps (start of each window)\n",
    "start_times = mains.index[i2:][st_te]\n",
    "anom_df = pd.DataFrame({\"start_time\": start_times, \"recon_error\": te_err, \"is_anomaly\": is_anom})\n",
    "anom_csv = os.path.join(OUTPUT_DIR, \"anomalies.csv\")\n",
    "anom_df.to_csv(anom_csv, index=False)\n",
    "print(f\"[INFO] Saved anomalies.csv -> {anom_csv}\")\n",
    "\n",
    "# Plot sample segment (first ~3 days of test)\n",
    "plot_len = min(3*24*60, len(mains) - i2)\n",
    "t_plot = mains.index[i2 : i2 + plot_len]\n",
    "y_plot = mains.values[i2 : i2 + plot_len]\n",
    "\n",
    "plt.figure(figsize=(12,4))\n",
    "plt.plot(t_plot, y_plot, label=\"Mains (test)\")\n",
    "for st_rel, flag in zip(st_te, is_anom):\n",
    "    if flag and st_rel < plot_len:\n",
    "        t0 = mains.index[i2 + st_rel]\n",
    "        t1 = mains.index[i2 + min(st_rel + AE_WINDOW, plot_len-1)]\n",
    "        plt.axvspan(t0, t1, color=\"red\", alpha=0.08)\n",
    "plt.title(\"Anomaly regions (autoencoder recon error)\")\n",
    "plt.xlabel(\"Time\"); plt.ylabel(\"Mains\")\n",
    "plt.tight_layout()\n",
    "anom_plot = os.path.join(OUTPUT_DIR, \"anomaly_plot.png\")\n",
    "plt.savefig(anom_plot, dpi=150); plt.close()\n",
    "print(f\"[INFO] Saved anomaly_plot.png -> {anom_plot}\")\n",
    "\n",
    "# Save AE model & preproc\n",
    "model_ae.save(os.path.join(OUTPUT_DIR, \"model_anomaly.h5\"))\n",
    "with open(os.path.join(OUTPUT_DIR, \"preprocessor_anomaly.pkl\"), \"wb\") as f:\n",
    "    pickle.dump({\"window\": AE_WINDOW, \"stride\": AE_STRIDE, \"scaler\": scaler_ae,\n",
    "                 \"mains_column_hint\": mains_saved}, f)\n",
    "\n",
    "print(\"\\n[DONE]\")\n",
    "print(\" Forecast artifacts:\")\n",
    "print(\"  - model_forecast.h5\")\n",
    "print(\"  - preprocessor_forecast.pkl\")\n",
    "print(\"  - forecast_config.yaml\")\n",
    "print(\"  - metrics_forecast.json\")\n",
    "print(\"  - forecast_plot.png\")\n",
    "print(\" Anomaly artifacts:\")\n",
    "print(\"  - model_anomaly.h5\")\n",
    "print(\"  - preprocessor_anomaly.pkl\")\n",
    "print(\"  - threshold_anom.json\")\n",
    "print(\"  - anomalies.csv\")\n",
    "print(\"  - anomaly_plot.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "487d1281-3b9d-49ce-8217-fc501ef3c20d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11 (moviepy)",
   "language": "python",
   "name": "py311"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
