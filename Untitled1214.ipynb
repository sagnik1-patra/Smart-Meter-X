{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0153d0c2-20e5-45e0-92a5-de203c949af8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] Wrote:\n",
      "C:\\Users\\sagni\\Downloads\\SmartMeterX\\infer.py\n",
      "C:\\Users\\sagni\\Downloads\\SmartMeterX\\gradio_app.py\n",
      "C:\\Users\\sagni\\Downloads\\SmartMeterX\\requirements_infer.txt\n",
      "\n",
      "Usage:\n",
      "CLI forecast:  python C:\\Users\\sagni\\Downloads\\SmartMeterX\\infer.py --task forecast --data_dir \"C:\\Users\\sagni\\Downloads\\SmartMeterX\\archive (2)\" --out_dir \"C:\\Users\\sagni\\Downloads\\SmartMeterX\"\n",
      "CLI anomaly :  python C:\\Users\\sagni\\Downloads\\SmartMeterX\\infer.py --task anomaly  --data_dir \"C:\\Users\\sagni\\Downloads\\SmartMeterX\\archive (2)\" --out_dir \"C:\\Users\\sagni\\Downloads\\SmartMeterX\"\n",
      "Gradio UI   :  python C:\\Users\\sagni\\Downloads\\SmartMeterX\\gradio_app.py  (then open http://127.0.0.1:7860/)\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# SmartMeterX â€” Inference scripts (forecast + anomaly) + optional Gradio UI\n",
    "# Writes: infer.py, gradio_app.py, requirements_infer.txt\n",
    "# ============================================================\n",
    "import os, textwrap, json\n",
    "\n",
    "OUTPUT_DIR = r\"C:\\Users\\sagni\\Downloads\\SmartMeterX\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "infer_py = r'''\n",
    "import os, glob, csv, json, warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "# -------------------------\n",
    "# Paths (artifacts live in this folder)\n",
    "# -------------------------\n",
    "ART_DIR = os.path.dirname(__file__)\n",
    "PREPROC_DISAGG = os.path.join(ART_DIR, \"preprocessor.pkl\")               # only for mains name hint (optional)\n",
    "PREPROC_FCST   = os.path.join(ART_DIR, \"preprocessor_forecast.pkl\")\n",
    "MODEL_FCST     = os.path.join(ART_DIR, \"model_forecast.h5\")\n",
    "\n",
    "PREPROC_AE     = os.path.join(ART_DIR, \"preprocessor_anomaly.pkl\")\n",
    "MODEL_AE       = os.path.join(ART_DIR, \"model_anomaly.h5\")\n",
    "THRESH_AE      = os.path.join(ART_DIR, \"threshold_anom.json\")\n",
    "\n",
    "# -------------------------\n",
    "# Robust CSV utils\n",
    "# -------------------------\n",
    "def robust_read_csv(path, expect_min_cols=2):\n",
    "    encs = [\"utf-8\",\"utf-8-sig\",\"cp1252\",\"latin1\"]\n",
    "    seps = [\",\",\";\",\"\\\\t\",\"|\"]\n",
    "    try:\n",
    "        with open(path, \"rb\") as f:\n",
    "            head = f.read(8192).decode(\"latin1\", errors=\"ignore\")\n",
    "        sn = csv.Sniffer().sniff(head)\n",
    "        if sn.delimiter in seps:\n",
    "            seps = [sn.delimiter] + [s for s in seps if s != sn.delimiter]\n",
    "    except Exception:\n",
    "        pass\n",
    "    last_err = None\n",
    "    for enc in encs:\n",
    "        for sep in seps:\n",
    "            try:\n",
    "                df = pd.read_csv(path, encoding=enc, sep=sep, engine=\"python\")\n",
    "                if df.shape[1] >= expect_min_cols:\n",
    "                    return df\n",
    "            except Exception as e:\n",
    "                last_err = e\n",
    "    raise RuntimeError(f\"Could not parse {path}. Last error: {last_err}\")\n",
    "\n",
    "def pick_time_column(cols):\n",
    "    cands = [\"timestamp\",\"time\",\"datetime\",\"date\",\"ts\",\"utc\",\"localtime\",\"index\"]\n",
    "    lmap = {c.lower(): c for c in cols}\n",
    "    for c in cands:\n",
    "        if c in lmap: return lmap[c]\n",
    "    return cols[0]\n",
    "\n",
    "def parse_time_column(df, tcol):\n",
    "    s = df[tcol]\n",
    "    if pd.api.types.is_numeric_dtype(s):\n",
    "        m = s.median()\n",
    "        try:\n",
    "            if m > 10**12:\n",
    "                return pd.to_datetime(s, unit=\"ms\", errors=\"coerce\")\n",
    "            elif m > 10**9:\n",
    "                return pd.to_datetime(s, unit=\"s\", errors=\"coerce\")\n",
    "        except Exception:\n",
    "            pass\n",
    "    return pd.to_datetime(s, errors=\"coerce\", infer_datetime_format=True)\n",
    "\n",
    "def looks_like_energy(colname:str):\n",
    "    ln = colname.lower()\n",
    "    return any(k in ln for k in [\"wh\",\"kwh\",\"energy\",\"consumption\",\"power\",\"mains\",\"aggregate\",\"total\",\"house\",\"use\"])\n",
    "\n",
    "def load_mains_series_only(data_dir, mains_hint:str|None):\n",
    "    all_csvs = [p for p in glob.glob(os.path.join(data_dir, \"**\", \"*\"), recursive=True)\n",
    "                if os.path.isfile(p) and p.lower().endswith(\".csv\")]\n",
    "    if not all_csvs:\n",
    "        raise FileNotFoundError(f\"No CSVs found under {data_dir}\")\n",
    "\n",
    "    best_series, best_sum = None, -1.0\n",
    "    hint_low = mains_hint.lower() if mains_hint else None\n",
    "\n",
    "    for path in all_csvs:\n",
    "        try:\n",
    "            df = robust_read_csv(path, expect_min_cols=2)\n",
    "            tcol = pick_time_column(list(df.columns))\n",
    "            df = df.dropna(subset=[tcol]).copy()\n",
    "            df[tcol] = parse_time_column(df, tcol)\n",
    "            df = df.dropna(subset=[tcol]).set_index(tcol).sort_index()\n",
    "            num_df = df.select_dtypes(include=[\"number\"])\n",
    "            if num_df.empty:\n",
    "                continue\n",
    "\n",
    "            cand_cols = []\n",
    "            if hint_low:\n",
    "                cand_cols = [c for c in num_df.columns if (hint_low in c.lower()) or c.lower().endswith(hint_low)]\n",
    "            if not cand_cols:\n",
    "                # fallback: mains-like columns in this file\n",
    "                cand_cols = [c for c in num_df.columns if looks_like_energy(c)]\n",
    "                # prefer the top-5 by variance\n",
    "                if len(cand_cols) > 5:\n",
    "                    var = num_df[cand_cols].var().sort_values(ascending=False)\n",
    "                    cand_cols = list(var.index[:5])\n",
    "\n",
    "            for c in cand_cols:\n",
    "                s = num_df[c].astype(\"float32\")\n",
    "                if looks_like_energy(c):\n",
    "                    s = s.resample(\"1T\").sum().astype(\"float32\")\n",
    "                else:\n",
    "                    s = s.resample(\"1T\").mean().astype(\"float32\")\n",
    "                tot = float(np.nansum(s.values))\n",
    "                if np.isfinite(tot) and tot > best_sum:\n",
    "                    best_sum = tot\n",
    "                    best_series = s\n",
    "        except Exception:\n",
    "            continue\n",
    "\n",
    "    if best_series is None:\n",
    "        raise RuntimeError(\"Could not find a mains-like series. Provide a tighter folder or check column names.\")\n",
    "    best_series = best_series.sort_index().ffill().bfill().fillna(0.0).astype(\"float32\")\n",
    "    return best_series\n",
    "\n",
    "def mains_hint_from_disagg():\n",
    "    try:\n",
    "        import pickle\n",
    "        with open(PREPROC_DISAGG, \"rb\") as f:\n",
    "            d = pickle.load(f)\n",
    "        return d.get(\"mains_column\", d.get(\"mains_col\"))\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "# -------------------------\n",
    "# Forecast next 24h (single shot)\n",
    "# -------------------------\n",
    "def forecast_next24(data_dir:str, out_dir:str|None=None):\n",
    "    import pickle\n",
    "    if out_dir is None:\n",
    "        out_dir = ART_DIR\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "    if not os.path.exists(PREPROC_FCST) or not os.path.exists(MODEL_FCST):\n",
    "        raise FileNotFoundError(\"Missing forecast artifacts (preprocessor_forecast.pkl/model_forecast.h5). Train first.\")\n",
    "\n",
    "    with open(PREPROC_FCST, \"rb\") as f:\n",
    "        pc = pickle.load(f)\n",
    "    hist = int(pc[\"hist_minutes\"]); horizon = int(pc[\"horizon_minutes\"])\n",
    "    feats = list(pc[\"feature_names\"])\n",
    "    scaler_X: StandardScaler = pc[\"scaler_X\"]\n",
    "    scaler_Y: StandardScaler = pc[\"scaler_Y\"]\n",
    "\n",
    "    # Load model safely\n",
    "    try:\n",
    "        model = tf.keras.models.load_model(MODEL_FCST, compile=False)\n",
    "    except Exception as e:\n",
    "        # final fallback\n",
    "        model = tf.keras.models.load_model(MODEL_FCST, custom_objects={\"mae\": tf.keras.metrics.mean_absolute_error}, compile=False)\n",
    "\n",
    "    mains_hint = pc.get(\"mains_column\") or mains_hint_from_disagg()\n",
    "    mains = load_mains_series_only(data_dir, mains_hint)\n",
    "\n",
    "    # Build features from the LAST 'hist' minutes\n",
    "    end = len(mains)\n",
    "    if end < hist + 10:\n",
    "        raise RuntimeError(f\"Not enough minutes ({end}) for {hist}-minute history.\")\n",
    "    idx = mains.index\n",
    "    df = pd.DataFrame(index=idx)\n",
    "    df[\"mains\"] = mains.values\n",
    "    h = idx.hour.values; d = idx.dayofweek.values\n",
    "    df[\"h_sin\"] = np.sin(2*np.pi*h/24.0).astype(\"float32\")\n",
    "    df[\"h_cos\"] = np.cos(2*np.pi*h/24.0).astype(\"float32\")\n",
    "    df[\"d_sin\"] = np.sin(2*np.pi*d/7.0).astype(\"float32\")\n",
    "    df[\"d_cos\"] = np.cos(2*np.pi*d/7.0).astype(\"float32\")\n",
    "\n",
    "    X_last = df[feats].iloc[-hist:].values.astype(\"float32\")\n",
    "    Xs = scaler_X.transform(X_last).reshape(1, hist, len(feats)).astype(\"float32\")\n",
    "\n",
    "    Y_pred_s = model.predict(Xs, verbose=0)        # (1, horizon, 1)\n",
    "    Y_pred = scaler_Y.inverse_transform(Y_pred_s.reshape(-1,1)).reshape(horizon)\n",
    "\n",
    "    # Build times for plot\n",
    "    t_hist = df.index[-hist:]\n",
    "    t_horz = pd.date_range(t_hist[-1] + pd.Timedelta(minutes=1), periods=horizon, freq=\"T\")\n",
    "    y_hist = df[\"mains\"].iloc[-hist:].values\n",
    "\n",
    "    # Plot & save\n",
    "    plt.figure(figsize=(12,4))\n",
    "    plt.plot(t_hist, y_hist, label=\"History (mains)\")\n",
    "    plt.plot(t_horz, Y_pred, label=\"Pred next 24h\")\n",
    "    plt.legend(); plt.title(\"SmartMeterX â€” Next-24h Forecast\")\n",
    "    plt.xlabel(\"Time\"); plt.ylabel(\"Mains\")\n",
    "    plt.tight_layout()\n",
    "    out_png = os.path.join(out_dir, \"forecast_next24.png\")\n",
    "    plt.savefig(out_png, dpi=150); plt.close()\n",
    "\n",
    "    # Return summary\n",
    "    out_json = {\n",
    "        \"status\": \"ok\",\n",
    "        \"hist_minutes\": hist,\n",
    "        \"horizon_minutes\": horizon,\n",
    "        \"plot_path\": out_png,\n",
    "    }\n",
    "    with open(os.path.join(out_dir, \"forecast_next24.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(out_json, f, indent=2)\n",
    "    return out_json\n",
    "\n",
    "# -------------------------\n",
    "# Anomaly detection (timeline)\n",
    "# -------------------------\n",
    "def detect_anomalies(data_dir:str, out_dir:str|None=None):\n",
    "    import pickle\n",
    "    if out_dir is None:\n",
    "        out_dir = ART_DIR\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "    if not (os.path.exists(PREPROC_AE) and os.path.exists(MODEL_AE) and os.path.exists(THRESH_AE)):\n",
    "        raise FileNotFoundError(\"Missing anomaly artifacts (preprocessor_anomaly.pkl/model_anomaly.h5/threshold_anom.json). Train first.\")\n",
    "\n",
    "    with open(PREPROC_AE, \"rb\") as f:\n",
    "        pa = pickle.load(f)\n",
    "    win = int(pa[\"window\"]); stride = int(pa[\"stride\"])\n",
    "    scaler: StandardScaler = pa[\"scaler\"]\n",
    "    mains_hint = pa.get(\"mains_column\") or pa.get(\"mains_column_hint\") or mains_hint_from_disagg()\n",
    "\n",
    "    with open(THRESH_AE, \"r\", encoding=\"utf-8\") as f:\n",
    "        th = json.load(f)\n",
    "    threshold = float(th[\"threshold\"])\n",
    "\n",
    "    # model\n",
    "    try:\n",
    "        model = tf.keras.models.load_model(MODEL_AE, compile=False)\n",
    "    except Exception:\n",
    "        model = tf.keras.models.load_model(MODEL_AE, compile=False)\n",
    "\n",
    "    mains = load_mains_series_only(data_dir, mains_hint)\n",
    "    vec = mains.values.astype(\"float32\")\n",
    "    N = len(vec)\n",
    "    if N < win + 10:\n",
    "        raise RuntimeError(f\"Not enough minutes ({N}) for AE window {win}.\")\n",
    "\n",
    "    starts = np.arange(0, N - win + 1, stride, dtype=int)\n",
    "    # stream in chunks for memory safety\n",
    "    batch = 1024\n",
    "    recon_err = np.zeros(len(starts), dtype=\"float32\")\n",
    "    for i in range(0, len(starts), batch):\n",
    "        sl = starts[i:i+batch]\n",
    "        X = np.stack([vec[s:s+win] for s in sl], axis=0).astype(\"float32\")[:, :, None]\n",
    "        Xs = scaler.transform(X.reshape(-1,1)).reshape(X.shape).astype(\"float32\")\n",
    "        rec = model.predict(Xs, verbose=0)\n",
    "        recon_err[i:i+batch] = np.mean((rec - Xs)**2, axis=(1,2))\n",
    "\n",
    "    is_anom = (recon_err >= threshold).astype(int)\n",
    "    times = mains.index[starts]\n",
    "    df = pd.DataFrame({\"start_time\": times, \"recon_error\": recon_err, \"is_anomaly\": is_anom})\n",
    "    out_csv = os.path.join(out_dir, \"anomalies.csv\")\n",
    "    df.to_csv(out_csv, index=False)\n",
    "\n",
    "    # timeline plot (mark spans)\n",
    "    plot_len = min(3*24*60, N)  # ~3 days or full if shorter\n",
    "    t_plot = mains.index[:plot_len]\n",
    "    y_plot = vec[:plot_len]\n",
    "    plt.figure(figsize=(12,4))\n",
    "    plt.plot(t_plot, y_plot, label=\"Mains\")\n",
    "    for st, flag in zip(starts, is_anom):\n",
    "        if flag and st < plot_len:\n",
    "            t0 = mains.index[st]\n",
    "            t1 = mains.index[min(st + win, plot_len-1)]\n",
    "            plt.axvspan(t0, t1, color=\"red\", alpha=0.08)\n",
    "    plt.title(\"SmartMeterX â€” Anomaly Timeline\")\n",
    "    plt.xlabel(\"Time\"); plt.ylabel(\"Mains\")\n",
    "    plt.tight_layout()\n",
    "    out_png = os.path.join(out_dir, \"anomaly_timeline.png\")\n",
    "    plt.savefig(out_png, dpi=150); plt.close()\n",
    "\n",
    "    out_json = {\n",
    "        \"status\": \"ok\",\n",
    "        \"threshold\": threshold,\n",
    "        \"csv_path\": out_csv,\n",
    "        \"plot_path\": out_png\n",
    "    }\n",
    "    with open(os.path.join(out_dir, \"anomaly_result.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(out_json, f, indent=2)\n",
    "    return out_json\n",
    "\n",
    "# -------------------------\n",
    "# CLI\n",
    "# -------------------------\n",
    "def _parse_args():\n",
    "    import argparse\n",
    "    p = argparse.ArgumentParser(description=\"SmartMeterX inference\")\n",
    "    p.add_argument(\"--task\", choices=[\"forecast\",\"anomaly\"], required=True, help=\"Which task to run\")\n",
    "    p.add_argument(\"--data_dir\", required=True, help=\"Folder containing AMPds CSVs\")\n",
    "    p.add_argument(\"--out_dir\", default=ART_DIR, help=\"Where to save outputs\")\n",
    "    return p.parse_args()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    args = _parse_args()\n",
    "    if args.task == \"forecast\":\n",
    "        res = forecast_next24(args.data_dir, args.out_dir)\n",
    "    else:\n",
    "        res = detect_anomalies(args.data_dir, args.out_dir)\n",
    "    print(json.dumps(res, indent=2, default=str))\n",
    "'''\n",
    "\n",
    "gradio_py = r'''\n",
    "import os, json, gradio as gr\n",
    "from infer import forecast_next24, detect_anomalies\n",
    "\n",
    "ART_DIR = os.path.dirname(__file__)\n",
    "\n",
    "def do_forecast(data_dir):\n",
    "    try:\n",
    "        res = forecast_next24(data_dir, ART_DIR)\n",
    "        return f\"OK\\nPlot: {res['plot_path']}\", res[\"plot_path\"]\n",
    "    except Exception as e:\n",
    "        return f\"ERROR: {e}\", None\n",
    "\n",
    "def do_anomaly(data_dir):\n",
    "    try:\n",
    "        res = detect_anomalies(data_dir, ART_DIR)\n",
    "        msg = f\"OK\\nCSV: {res['csv_path']}\\nPlot: {res['plot_path']}\"\n",
    "        return msg, res[\"plot_path\"]\n",
    "    except Exception as e:\n",
    "        return f\"ERROR: {e}\", None\n",
    "\n",
    "with gr.Blocks(title=\"SmartMeterX Inference\") as demo:\n",
    "    gr.Markdown(\"# SmartMeterX â€” Forecast & Anomaly\")\n",
    "    with gr.Tab(\"Forecast next 24h\"):\n",
    "        data_dir_f = gr.Textbox(label=\"AMPds folder\", value=r\"C:\\Users\\sagni\\Downloads\\SmartMeterX\\archive (2)\")\n",
    "        out_txt_f = gr.Textbox(label=\"Status / Paths\")\n",
    "        out_img_f = gr.Image(label=\"Forecast Plot\")\n",
    "        btn_f = gr.Button(\"Run Forecast\")\n",
    "        btn_f.click(fn=do_forecast, inputs=data_dir_f, outputs=[out_txt_f, out_img_f])\n",
    "\n",
    "    with gr.Tab(\"Anomaly Detection\"):\n",
    "        data_dir_a = gr.Textbox(label=\"AMPds folder\", value=r\"C:\\Users\\sagni\\Downloads\\SmartMeterX\\archive (2)\")\n",
    "        out_txt_a = gr.Textbox(label=\"Status / Paths\")\n",
    "        out_img_a = gr.Image(label=\"Anomaly Timeline\")\n",
    "        btn_a = gr.Button(\"Run Anomaly\")\n",
    "        btn_a.click(fn=do_anomaly, inputs=data_dir_a, outputs=[out_txt_a, out_img_a])\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    demo.launch(server_name=\"127.0.0.1\", server_port=7860, show_api=False)\n",
    "'''\n",
    "\n",
    "reqs = \"\"\"numpy\n",
    "pandas\n",
    "matplotlib\n",
    "scikit-learn\n",
    "tensorflow\n",
    "gradio\n",
    "\"\"\"\n",
    "\n",
    "with open(os.path.join(OUTPUT_DIR, \"infer.py\"), \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(textwrap.dedent(infer_py).strip()+\"\\n\")\n",
    "\n",
    "with open(os.path.join(OUTPUT_DIR, \"gradio_app.py\"), \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(textwrap.dedent(gradio_py).strip()+\"\\n\")\n",
    "\n",
    "with open(os.path.join(OUTPUT_DIR, \"requirements_infer.txt\"), \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(reqs)\n",
    "\n",
    "print(\"[OK] Wrote:\")\n",
    "print(os.path.join(OUTPUT_DIR, \"infer.py\"))\n",
    "print(os.path.join(OUTPUT_DIR, \"gradio_app.py\"))\n",
    "print(os.path.join(OUTPUT_DIR, \"requirements_infer.txt\"))\n",
    "\n",
    "print(\"\\nUsage:\")\n",
    "print(r'CLI forecast:  python C:\\Users\\sagni\\Downloads\\SmartMeterX\\infer.py --task forecast --data_dir \"C:\\Users\\sagni\\Downloads\\SmartMeterX\\archive (2)\" --out_dir \"C:\\Users\\sagni\\Downloads\\SmartMeterX\"')\n",
    "print(r'CLI anomaly :  python C:\\Users\\sagni\\Downloads\\SmartMeterX\\infer.py --task anomaly  --data_dir \"C:\\Users\\sagni\\Downloads\\SmartMeterX\\archive (2)\" --out_dir \"C:\\Users\\sagni\\Downloads\\SmartMeterX\"')\n",
    "print(r'Gradio UI   :  python C:\\Users\\sagni\\Downloads\\SmartMeterX\\gradio_app.py  (then open http://127.0.0.1:7860/)')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4618898a-4033-4946-8633-db6030f65699",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11 (moviepy)",
   "language": "python",
   "name": "py311"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
