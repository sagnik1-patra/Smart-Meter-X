{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4b24c8c6-31be-40cb-a4b4-40cfc640f2e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Loaded Climate_HourlyWeather.csv -> cols=15, rows=1381\n",
      "[INFO] Loaded Electricity_B1E.csv -> cols=11, rows=1051200\n",
      "[INFO] Loaded Electricity_B2E.csv -> cols=11, rows=1051200\n",
      "[INFO] Loaded Electricity_Billing.csv -> cols=14, rows=1056961\n",
      "[INFO] Loaded Electricity_BME.csv -> cols=11, rows=1051200\n",
      "[INFO] Loaded Electricity_CDE.csv -> cols=11, rows=1051200\n",
      "[INFO] Loaded Electricity_CWE.csv -> cols=11, rows=1051200\n",
      "[INFO] Loaded Electricity_DNE.csv -> cols=11, rows=1051200\n",
      "[INFO] Loaded Electricity_DWE.csv -> cols=11, rows=1051200\n",
      "[INFO] Loaded Electricity_EBE.csv -> cols=11, rows=1051200\n",
      "[INFO] Loaded Electricity_EQE.csv -> cols=11, rows=1051200\n",
      "[INFO] Loaded Electricity_FGE.csv -> cols=11, rows=1051200\n",
      "[INFO] Loaded Electricity_FRE.csv -> cols=11, rows=1051200\n",
      "[INFO] Loaded Electricity_GRE.csv -> cols=11, rows=1051200\n",
      "[INFO] Loaded Electricity_HPE.csv -> cols=11, rows=1051200\n",
      "[INFO] Loaded Electricity_HTE.csv -> cols=11, rows=1051200\n",
      "[INFO] Loaded Electricity_I.csv -> cols=23, rows=1051200\n",
      "[INFO] Loaded Electricity_Monthly.csv -> cols=4, rows=1006561\n",
      "[INFO] Loaded Electricity_OFE.csv -> cols=11, rows=1051200\n",
      "[INFO] Loaded Electricity_OUE.csv -> cols=11, rows=1051200\n",
      "[INFO] Loaded Electricity_P.csv -> cols=23, rows=1051200\n",
      "[INFO] Loaded Electricity_Q.csv -> cols=23, rows=1051200\n",
      "[INFO] Loaded Electricity_RSE.csv -> cols=11, rows=1051200\n",
      "[INFO] Loaded Electricity_S.csv -> cols=23, rows=1051200\n",
      "[INFO] Loaded Electricity_TVE.csv -> cols=11, rows=1051200\n",
      "[INFO] Loaded Electricity_UTE.csv -> cols=11, rows=1051200\n",
      "[INFO] Loaded Electricity_WHE.csv -> cols=11, rows=1051200\n",
      "[INFO] Loaded Electricity_WOE.csv -> cols=11, rows=1051200\n",
      "[INFO] Loaded NaturalGas_Billing.csv -> cols=10, rows=1046881\n",
      "[INFO] Loaded NaturalGas_FRG.csv -> cols=3, rows=1051200\n",
      "[INFO] Loaded NaturalGas_Monthly.csv -> cols=2, rows=1006561\n",
      "[INFO] Loaded NaturalGas_WHG.csv -> cols=3, rows=1051200\n",
      "[INFO] Loaded Water_Billing.csv -> cols=5, rows=1\n",
      "[INFO] Loaded Water_DWW.csv -> cols=2, rows=1051200\n",
      "[INFO] Loaded Water_HTW.csv -> cols=3, rows=1051200\n",
      "[INFO] Loaded Water_WHW.csv -> cols=3, rows=1051200\n",
      "[INFO] Combined frame: (1085703, 384)\n",
      "[INFO] Selected mains: Water_Billing__Total Fees\n",
      "[INFO] Selected appliances: ['Electricity_WHE__St', 'Electricity_WHE__Pt', 'Electricity_RSE__St', 'Electricity_RSE__Pt', 'Electricity_HPE__St']\n",
      "[INFO] Windowed shapes: (47484, 256, 5) (47484, 256, 5) (10163, 256, 5) (10163, 256, 5) (10163, 256, 5) (10163, 256, 5)\n",
      "Epoch 1/40\n",
      "\u001b[1m742/742\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 32ms/step - loss: 0.8996 - mae: 0.8996 - val_loss: 2.0160 - val_mae: 2.0160\n",
      "Epoch 2/40\n",
      "\u001b[1m742/742\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 29ms/step - loss: 0.8797 - mae: 0.8797 - val_loss: 2.0145 - val_mae: 2.0145\n",
      "Epoch 3/40\n",
      "\u001b[1m742/742\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 27ms/step - loss: 0.8796 - mae: 0.8796 - val_loss: 2.0150 - val_mae: 2.0150\n",
      "Epoch 4/40\n",
      "\u001b[1m742/742\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 27ms/step - loss: 0.8796 - mae: 0.8796 - val_loss: 2.0138 - val_mae: 2.0138\n",
      "Epoch 5/40\n",
      "\u001b[1m742/742\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 28ms/step - loss: 0.8796 - mae: 0.8796 - val_loss: 2.0045 - val_mae: 2.0045\n",
      "Epoch 6/40\n",
      "\u001b[1m742/742\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 27ms/step - loss: 0.8797 - mae: 0.8797 - val_loss: 2.0149 - val_mae: 2.0149\n",
      "Epoch 7/40\n",
      "\u001b[1m742/742\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 29ms/step - loss: 0.8796 - mae: 0.8796 - val_loss: 2.0149 - val_mae: 2.0149\n",
      "Epoch 8/40\n",
      "\u001b[1m742/742\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 27ms/step - loss: 0.8796 - mae: 0.8796 - val_loss: 2.0149 - val_mae: 2.0149\n",
      "Epoch 9/40\n",
      "\u001b[1m742/742\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 28ms/step - loss: 0.8796 - mae: 0.8796 - val_loss: 2.0148 - val_mae: 2.0148\n",
      "Epoch 10/40\n",
      "\u001b[1m742/742\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 29ms/step - loss: 0.8797 - mae: 0.8797 - val_loss: 2.0144 - val_mae: 2.0144\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Saved metrics.json\n",
      "[INFO] Saved preprocessor.pkl\n",
      "[INFO] Saved model_disagg.h5 -> C:\\Users\\sagni\\Downloads\\SmartMeterX\\model_disagg.h5\n",
      "[INFO] Saved model_config.yaml -> C:\\Users\\sagni\\Downloads\\SmartMeterX\\model_config.yaml\n",
      "\n",
      "[DONE] Artifacts saved in: C:\\Users\\sagni\\Downloads\\SmartMeterX\n",
      " - preprocessor.pkl\n",
      " - model_disagg.h5\n",
      " - model_config.yaml\n",
      " - metrics.json\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# SmartMeterX (AMPds) — Auto-load CSVs, train 1D-CNN disaggregator,\n",
    "# save: preprocessor.pkl, model_disagg.h5, model_config.yaml, metrics.json\n",
    "# ============================================================\n",
    "import os, re, csv, json, math, pickle, warnings, glob, random\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# --------------------------\n",
    "# Paths (YOUR EXACT FOLDERS)\n",
    "# --------------------------\n",
    "DATA_DIR   = r\"C:\\Users\\sagni\\Downloads\\SmartMeterX\\archive (2)\"\n",
    "OUTPUT_DIR = r\"C:\\Users\\sagni\\Downloads\\SmartMeterX\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# --------------------------\n",
    "# Reproducibility\n",
    "# --------------------------\n",
    "SEED = 42\n",
    "random.seed(SEED); np.random.seed(SEED); tf.random.set_seed(SEED)\n",
    "\n",
    "# --------------------------\n",
    "# Helpers: robust CSV reader & timestamp inference\n",
    "# --------------------------\n",
    "def robust_read_csv(path, expect_min_cols=2):\n",
    "    encs = [\"utf-8\",\"utf-8-sig\",\"cp1252\",\"latin1\"]\n",
    "    seps = [\",\",\";\",\"\\t\",\"|\"]\n",
    "    # Try sniffing delimiter from header preview\n",
    "    try:\n",
    "        with open(path, \"rb\") as f:\n",
    "            head = f.read(8192).decode(\"latin1\", errors=\"ignore\")\n",
    "        sn = csv.Sniffer().sniff(head)\n",
    "        if sn.delimiter in seps:\n",
    "            seps = [sn.delimiter] + [s for s in seps if s != sn.delimiter]\n",
    "    except Exception:\n",
    "        pass\n",
    "    last_err = None\n",
    "    for enc in encs:\n",
    "        for sep in seps:\n",
    "            try:\n",
    "                df = pd.read_csv(path, encoding=enc, sep=sep, engine=\"python\")\n",
    "                if df.shape[1] >= expect_min_cols:\n",
    "                    return df\n",
    "            except Exception as e:\n",
    "                last_err = e\n",
    "    raise RuntimeError(f\"Could not parse {path}. Last error: {last_err}\")\n",
    "\n",
    "def pick_time_column(cols):\n",
    "    # Common timestamp names\n",
    "    cand = [\"timestamp\",\"time\",\"datetime\",\"date\",\"ts\",\"utc\",\"localtime\",\"index\"]\n",
    "    lmap = {c.lower(): c for c in cols}\n",
    "    for c in cand:\n",
    "        if c in lmap: return lmap[c]\n",
    "    # fallback: if first col looks like time\n",
    "    return cols[0]\n",
    "\n",
    "def parse_time_column(df, tcol):\n",
    "    # Try pandas to_datetime with multiple formats\n",
    "    # AMPds often uses ISO or numeric epoch-like fields\n",
    "    s = df[tcol]\n",
    "    # If numeric epoch-ish (seconds/minutes), try to detect scale\n",
    "    if pd.api.types.is_numeric_dtype(s):\n",
    "        # Heuristic: large numbers ~ epoch seconds/ms\n",
    "        m = s.median()\n",
    "        try:\n",
    "            if m > 10**12:  # ms\n",
    "                return pd.to_datetime(s, unit=\"ms\", errors=\"coerce\")\n",
    "            elif m > 10**9:  # s\n",
    "                return pd.to_datetime(s, unit=\"s\", errors=\"coerce\")\n",
    "            else:\n",
    "                # Could be minute index offset; try as datetime anyway\n",
    "                return pd.to_datetime(s, errors=\"coerce\", infer_datetime_format=True)\n",
    "        except Exception:\n",
    "            pass\n",
    "    # String-like datetimes\n",
    "    return pd.to_datetime(s, errors=\"coerce\", infer_datetime_format=True)\n",
    "\n",
    "def add_prefix(df, prefix):\n",
    "    # Avoid duplicate names across files\n",
    "    newcols = {}\n",
    "    for c in df.columns:\n",
    "        if c.lower() in [\"timestamp\",\"time\",\"datetime\",\"date\",\"ts\",\"utc\",\"localtime\",\"index\"]:\n",
    "            newcols[c] = c  # keep time\n",
    "        else:\n",
    "            newcols[c] = f\"{prefix}__{c}\"\n",
    "    return df.rename(columns=newcols)\n",
    "\n",
    "# --------------------------\n",
    "# 1) Load every CSV in folder (ignore HTML), outer-join on minute index\n",
    "# --------------------------\n",
    "all_csvs = [p for p in glob.glob(os.path.join(DATA_DIR, \"**\", \"*\"), recursive=True)\n",
    "            if os.path.isfile(p) and p.lower().endswith(\".csv\")]\n",
    "\n",
    "if not all_csvs:\n",
    "    raise FileNotFoundError(f\"No CSVs found under: {DATA_DIR}\")\n",
    "\n",
    "frames = []\n",
    "for path in all_csvs:\n",
    "    try:\n",
    "        df = robust_read_csv(path, expect_min_cols=2)\n",
    "        tcol = pick_time_column(list(df.columns))\n",
    "        df = df.dropna(subset=[tcol]).copy()\n",
    "        df[tcol] = parse_time_column(df, tcol)\n",
    "        df = df.dropna(subset=[tcol]).copy()\n",
    "        df = df.set_index(tcol).sort_index()\n",
    "\n",
    "        # Keep only numeric channels\n",
    "        num_df = df.select_dtypes(include=[\"number\"]).copy()\n",
    "        if num_df.empty:\n",
    "            continue\n",
    "        # Prefix to keep uniqueness\n",
    "        stem = os.path.splitext(os.path.basename(path))[0]\n",
    "        num_df = add_prefix(num_df, stem)\n",
    "\n",
    "        # Resample to 1-minute mean/sum (AMPds is minutely already; mean is harmless)\n",
    "        # Choose mean by default; if a column name contains 'energy' or 'wh', use sum.\n",
    "        def agg_func(colname):\n",
    "            ln = colname.lower()\n",
    "            if any(k in ln for k in [\"wh\",\"kwh\",\"energy\",\"consumption\"]):\n",
    "                return \"sum\"\n",
    "            return \"mean\"\n",
    "        # Build dict of agg funcs\n",
    "        aggmap = {c: agg_func(c) for c in num_df.columns}\n",
    "        num_df = num_df.resample(\"1T\").agg(aggmap)\n",
    "\n",
    "        frames.append(num_df)\n",
    "        print(f\"[INFO] Loaded {os.path.basename(path)} -> cols={num_df.shape[1]}, rows={num_df.shape[0]}\")\n",
    "    except Exception as e:\n",
    "        print(f\"[WARN] Skipped {os.path.basename(path)}: {e}\")\n",
    "\n",
    "if not frames:\n",
    "    raise RuntimeError(\"No usable numeric CSVs could be parsed.\")\n",
    "\n",
    "data = pd.concat(frames, axis=1).sort_index()\n",
    "# Drop columns entirely empty\n",
    "data = data.dropna(axis=1, how=\"all\")\n",
    "# Fill gaps (forward/back fill then zeros for any leading/trailing)\n",
    "data = data.ffill().bfill().fillna(0.0)\n",
    "\n",
    "print(\"[INFO] Combined frame:\", data.shape)\n",
    "\n",
    "# --------------------------\n",
    "# 2) Choose mains + appliances automatically\n",
    "#    - mains: column with the highest total energy/mean that looks like aggregate\n",
    "#    - appliances: top-N by total energy excluding mains\n",
    "# --------------------------\n",
    "def looks_like_mains(colname):\n",
    "    ln = colname.lower()\n",
    "    return any(k in ln for k in [\n",
    "        \"mains\",\"aggregate\",\"total\",\"house\",\"net\",\"use\",\"whole\",\"summed\",\"site\",\"mainspower\"\n",
    "    ])\n",
    "\n",
    "energy_by_col = data.sum()  # assuming minutely mean/sum; proportional to energy\n",
    "# Preferred: candidates that look like mains; else pick the absolute max\n",
    "mains_candidates = [c for c in data.columns if looks_like_mains(c)]\n",
    "if mains_candidates:\n",
    "    mains_col = max(mains_candidates, key=lambda c: energy_by_col.get(c, 0))\n",
    "else:\n",
    "    mains_col = energy_by_col.idxmax()\n",
    "\n",
    "# Pick top appliances (exclude mains). Limit to 5 for a fast baseline.\n",
    "appliance_pool = [c for c in data.columns if c != mains_col]\n",
    "appliance_pool_sorted = sorted(appliance_pool, key=lambda c: energy_by_col.get(c, 0), reverse=True)\n",
    "APPLIANCE_COLS = appliance_pool_sorted[:5] if len(appliance_pool_sorted) > 5 else appliance_pool_sorted\n",
    "\n",
    "if len(APPLIANCE_COLS) == 0:\n",
    "    raise RuntimeError(\"No appliance-like columns found after parsing. Check CSV contents.\")\n",
    "\n",
    "print(\"[INFO] Selected mains:\", mains_col)\n",
    "print(\"[INFO] Selected appliances:\", APPLIANCE_COLS)\n",
    "\n",
    "# --------------------------\n",
    "# 3) Build features: mains + calendar (sin/cos hour & day)\n",
    "# --------------------------\n",
    "def calendar_features(index):\n",
    "    # hour of day (0-23)\n",
    "    h = index.hour.values\n",
    "    # day of week (0-6)\n",
    "    d = index.dayofweek.values\n",
    "    # sin/cos transforms\n",
    "    h_sin = np.sin(2*np.pi*h/24.0)\n",
    "    h_cos = np.cos(2*np.pi*h/24.0)\n",
    "    d_sin = np.sin(2*np.pi*d/7.0)\n",
    "    d_cos = np.cos(2*np.pi*d/7.0)\n",
    "    return np.column_stack([h_sin, h_cos, d_sin, d_cos]).astype(\"float32\")\n",
    "\n",
    "features = pd.DataFrame(index=data.index)\n",
    "features[\"mains\"] = data[mains_col].astype(\"float32\")\n",
    "cal = calendar_features(features.index)\n",
    "for i, name in enumerate([\"h_sin\",\"h_cos\",\"d_sin\",\"d_cos\"]):\n",
    "    features[name] = cal[:, i]\n",
    "\n",
    "targets = data[APPLIANCE_COLS].astype(\"float32\").copy()\n",
    "\n",
    "# --------------------------\n",
    "# 4) Time-based split: 70/15/15\n",
    "# --------------------------\n",
    "n = len(features)\n",
    "i1 = int(n * 0.70); i2 = int(n * 0.85)\n",
    "X_train_df = features.iloc[:i1]\n",
    "X_val_df   = features.iloc[i1:i2]\n",
    "X_test_df  = features.iloc[i2:]\n",
    "\n",
    "y_train_df = targets.iloc[:i1]\n",
    "y_val_df   = targets.iloc[i1:i2]\n",
    "y_test_df  = targets.iloc[i2:]\n",
    "\n",
    "# --------------------------\n",
    "# 5) Windowed sequences for 1D-CNN (seq2seq)\n",
    "# --------------------------\n",
    "WINDOW = 256   # ~ 4.3 hours at 1-minute sampling\n",
    "STRIDE = 16    # ~ 16 minutes hop\n",
    "\n",
    "def make_windows(Xdf, ydf, window=WINDOW, stride=STRIDE):\n",
    "    X = Xdf.values\n",
    "    Y = ydf.values\n",
    "    N, Din = X.shape\n",
    "    Dout = Y.shape[1]\n",
    "    xs, ys = [], []\n",
    "    for start in range(0, N - window + 1, stride):\n",
    "        end = start + window\n",
    "        xs.append(X[start:end, :])\n",
    "        ys.append(Y[start:end, :])\n",
    "    if not xs:\n",
    "        return None, None\n",
    "    return np.stack(xs), np.stack(ys)\n",
    "\n",
    "Xtr_win, ytr_win = make_windows(X_train_df, y_train_df)\n",
    "Xva_win, yva_win = make_windows(X_val_df,   y_val_df)\n",
    "Xte_win, yte_win = make_windows(X_test_df,  y_test_df)\n",
    "\n",
    "for nm, arr in [(\"Xtr\", Xtr_win), (\"ytr\", ytr_win), (\"Xva\", Xva_win), (\"yva\", yva_win), (\"Xte\", Xte_win), (\"yte\", yte_win)]:\n",
    "    if arr is None:\n",
    "        raise RuntimeError(f\"{nm} windows are empty. Try reducing WINDOW/STRIDE or check data.\")\n",
    "print(\"[INFO] Windowed shapes:\", Xtr_win.shape, ytr_win.shape, Xva_win.shape, yva_win.shape, Xte_win.shape, yte_win.shape)\n",
    "\n",
    "# --------------------------\n",
    "# 6) Scale features and targets (fit on train windows)\n",
    "# --------------------------\n",
    "# Flatten windows for scaler fit\n",
    "Xtr_flat = Xtr_win.reshape(-1, Xtr_win.shape[-1])\n",
    "ytr_flat = ytr_win.reshape(-1, ytr_win.shape[-1])\n",
    "\n",
    "scaler_X = StandardScaler().fit(Xtr_flat)\n",
    "scaler_Y = StandardScaler().fit(ytr_flat)\n",
    "\n",
    "def scale_xy(Xw, Yw):\n",
    "    X = scaler_X.transform(Xw.reshape(-1, Xw.shape[-1])).reshape(Xw.shape)\n",
    "    Y = scaler_Y.transform(Yw.reshape(-1, Yw.shape[-1])).reshape(Yw.shape)\n",
    "    return X.astype(\"float32\"), Y.astype(\"float32\")\n",
    "\n",
    "Xtr, ytr = scale_xy(Xtr_win, ytr_win)\n",
    "Xva, yva = scale_xy(Xva_win, yva_win)\n",
    "Xte, yte = scale_xy(Xte_win, yte_win)\n",
    "\n",
    "# --------------------------\n",
    "# 7) Build 1D-CNN seq2seq disaggregator\n",
    "# --------------------------\n",
    "Din  = Xtr.shape[-1]             # mains + 4 calendar = 5\n",
    "Dout = ytr.shape[-1]             # number of appliances\n",
    "\n",
    "def build_disagg_cnn(window=WINDOW, in_ch=Din, out_ch=Dout):\n",
    "    inp = keras.Input(shape=(window, in_ch))\n",
    "    x = layers.Conv1D(64, 5, padding=\"same\", activation=\"relu\")(inp)\n",
    "    x = layers.Conv1D(64, 5, padding=\"same\", activation=\"relu\")(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Conv1D(96, 5, padding=\"same\", activation=\"relu\")(x)\n",
    "    x = layers.Dropout(0.2)(x)\n",
    "    x = layers.Conv1D(out_ch, 1, padding=\"same\", activation=None)(x)  # linear output\n",
    "    # Non-negativity: we can clamp at 0 during postprocess; keep linear for training stability\n",
    "    model = keras.Model(inp, x, name=\"smartmeterx_disagg_cnn\")\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(1e-3),\n",
    "        loss=\"mae\",\n",
    "        metrics=[keras.metrics.MeanAbsoluteError(name=\"mae\")]\n",
    "    )\n",
    "    return model\n",
    "\n",
    "model = build_disagg_cnn()\n",
    "early = keras.callbacks.EarlyStopping(monitor=\"val_mae\", mode=\"min\", patience=5, restore_best_weights=True)\n",
    "hist = model.fit(\n",
    "    Xtr, ytr,\n",
    "    validation_data=(Xva, yva),\n",
    "    epochs=40, batch_size=64,\n",
    "    callbacks=[early],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# --------------------------\n",
    "# 8) Evaluate on test (MAE per appliance, sMAPE)\n",
    "# --------------------------\n",
    "# Predict (scaled), invert scaling\n",
    "yte_pred_scaled = model.predict(Xte, batch_size=128, verbose=0)\n",
    "# flatten and invert scaler_Y\n",
    "yte_pred_flat = yte_pred_scaled.reshape(-1, Dout)\n",
    "yte_true_flat = yte.reshape(-1, Dout)\n",
    "yte_pred_unscaled = scaler_Y.inverse_transform(yte_pred_flat)\n",
    "yte_true_unscaled = scaler_Y.inverse_transform(yte_true_flat)\n",
    "\n",
    "# Non-negativity clamp\n",
    "yte_pred_unscaled = np.clip(yte_pred_unscaled, 0.0, None)\n",
    "\n",
    "def smape(a, f, eps=1e-6):\n",
    "    return 100.0 * np.mean(2.0 * np.abs(f - a) / (np.abs(a) + np.abs(f) + eps))\n",
    "\n",
    "mae_per_appliance = {}\n",
    "smape_per_appliance = {}\n",
    "for i, ap in enumerate(APPLIANCE_COLS):\n",
    "    mae_i = mean_absolute_error(yte_true_unscaled[:, i], yte_pred_unscaled[:, i])\n",
    "    smape_i = smape(yte_true_unscaled[:, i], yte_pred_unscaled[:, i])\n",
    "    mae_per_appliance[ap] = float(mae_i)\n",
    "    smape_per_appliance[ap] = float(smape_i)\n",
    "\n",
    "# Aggregate (sum over appliances)\n",
    "mae_avg = float(np.mean(list(mae_per_appliance.values())))\n",
    "smape_avg = float(np.mean(list(smape_per_appliance.values())))\n",
    "\n",
    "metrics = {\n",
    "    \"window\": int(WINDOW),\n",
    "    \"stride\": int(STRIDE),\n",
    "    \"num_appliances\": int(Dout),\n",
    "    \"appliances\": APPLIANCE_COLS,\n",
    "    \"mains_column\": mains_col,\n",
    "    \"mae_per_appliance\": mae_per_appliance,\n",
    "    \"smape_per_appliance\": smape_per_appliance,\n",
    "    \"mae_mean\": mae_avg,\n",
    "    \"smape_mean\": smape_avg\n",
    "}\n",
    "with open(os.path.join(OUTPUT_DIR, \"metrics.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(metrics, f, indent=2)\n",
    "print(\"[INFO] Saved metrics.json\")\n",
    "\n",
    "# --------------------------\n",
    "# 9) Save artifacts (.pkl, .h5, .yaml)\n",
    "# --------------------------\n",
    "preproc = {\n",
    "    \"mains_column\": mains_col,\n",
    "    \"appliance_columns\": APPLIANCE_COLS,\n",
    "    \"feature_columns\": [\"mains\",\"h_sin\",\"h_cos\",\"d_sin\",\"d_cos\"],\n",
    "    \"window\": WINDOW,\n",
    "    \"stride\": STRIDE,\n",
    "    \"scaler_X\": scaler_X,\n",
    "    \"scaler_Y\": scaler_Y,\n",
    "    \"sampling\": \"1T\",\n",
    "    \"notes\": \"X = [mains + calendar]; Y = appliances; scalers fitted on flattened train windows.\"\n",
    "}\n",
    "with open(os.path.join(OUTPUT_DIR, \"preprocessor.pkl\"), \"wb\") as f:\n",
    "    pickle.dump(preproc, f)\n",
    "print(\"[INFO] Saved preprocessor.pkl\")\n",
    "\n",
    "# Save Keras model in HDF5 legacy format (as requested)\n",
    "h5_path = os.path.join(OUTPUT_DIR, \"model_disagg.h5\")\n",
    "model.save(h5_path)\n",
    "print(\"[INFO] Saved model_disagg.h5 ->\", h5_path)\n",
    "\n",
    "# YAML config dump (paths + hyperparams)\n",
    "config = {\n",
    "    \"project\": \"SmartMeterX — NILM Disaggregation (AMPds auto-loader)\",\n",
    "    \"paths\": {\n",
    "        \"data_dir\": DATA_DIR,\n",
    "        \"output_dir\": OUTPUT_DIR\n",
    "    },\n",
    "    \"selection\": {\n",
    "        \"mains_column\": mains_col,\n",
    "        \"appliance_columns\": APPLIANCE_COLS\n",
    "    },\n",
    "    \"preprocessing\": {\n",
    "        \"resample\": \"1T\",\n",
    "        \"calendar_features\": [\"h_sin\",\"h_cos\",\"d_sin\",\"d_cos\"],\n",
    "        \"fill\": \"ffill->bfill->0\",\n",
    "        \"scalers\": {\"X\": \"StandardScaler\", \"Y\": \"StandardScaler\"}\n",
    "    },\n",
    "    \"windows\": {\"length\": WINDOW, \"stride\": STRIDE},\n",
    "    \"model\": {\n",
    "        \"type\": \"1D-CNN seq2seq\",\n",
    "        \"layers\": [\n",
    "            {\"Conv1D\": {\"filters\": 64, \"kernel_size\": 5, \"activation\": \"relu\", \"padding\": \"same\"}},\n",
    "            {\"Conv1D\": {\"filters\": 64, \"kernel_size\": 5, \"activation\": \"relu\", \"padding\": \"same\"}},\n",
    "            {\"BatchNorm\": {}},\n",
    "            {\"Conv1D\": {\"filters\": 96, \"kernel_size\": 5, \"activation\": \"relu\", \"padding\": \"same\"}},\n",
    "            {\"Dropout\": 0.2},\n",
    "            {\"Conv1D\": {\"filters\": len(APPLIANCE_COLS), \"kernel_size\": 1, \"activation\": \"linear\", \"padding\": \"same\"}}\n",
    "        ],\n",
    "        \"optimizer\": \"adam\",\n",
    "        \"learning_rate\": 1e-3,\n",
    "        \"loss\": \"mae\",\n",
    "        \"metrics\": [\"mae\"],\n",
    "        \"epochs\": 40,\n",
    "        \"batch_size\": 64,\n",
    "        \"early_stopping\": {\"monitor\":\"val_mae\",\"mode\":\"min\",\"patience\":5}\n",
    "    }\n",
    "}\n",
    "yaml_path = os.path.join(OUTPUT_DIR, \"model_config.yaml\")\n",
    "try:\n",
    "    import yaml\n",
    "    with open(yaml_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        yaml.safe_dump(config, f, sort_keys=False)\n",
    "except Exception:\n",
    "    # minimal YAML fallback\n",
    "    def to_yaml(d, indent=0):\n",
    "        lines, pad = [], \"  \" * indent\n",
    "        if isinstance(d, dict):\n",
    "            for k, v in d.items():\n",
    "                if isinstance(v, (dict, list)):\n",
    "                    lines.append(f\"{pad}{k}:\")\n",
    "                    lines.extend(to_yaml(v, indent+1))\n",
    "                else:\n",
    "                    lines.append(f\"{pad}{k}: {repr(v)}\")\n",
    "        elif isinstance(d, list):\n",
    "            for it in d:\n",
    "                if isinstance(it, (dict, list)):\n",
    "                    lines.append(f\"{pad}-\")\n",
    "                    lines.extend(to_yaml(it, indent+1))\n",
    "                else:\n",
    "                    lines.append(f\"{pad}- {repr(it)}\")\n",
    "        return lines\n",
    "    with open(yaml_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(\"\\n\".join(to_yaml(config)))\n",
    "print(\"[INFO] Saved model_config.yaml ->\", yaml_path)\n",
    "\n",
    "print(\"\\n[DONE] Artifacts saved in:\", OUTPUT_DIR)\n",
    "print(\" - preprocessor.pkl\")\n",
    "print(\" - model_disagg.h5\")\n",
    "print(\" - model_config.yaml\")\n",
    "print(\" - metrics.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b2af5b4-3e12-4432-ab22-066fb150c9da",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11 (moviepy)",
   "language": "python",
   "name": "py311"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
